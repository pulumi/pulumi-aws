// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.aws.kinesis.inputs;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Import;
import java.lang.Boolean;
import java.lang.Integer;
import java.lang.String;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;


public final class FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs extends com.pulumi.resources.ResourceArgs {

    public static final FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs Empty = new FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs();

    /**
     * The Hadoop Distributed File System (HDFS) block size. This is useful if you intend to copy the data from Amazon S3 to HDFS before querying. The default is 256 MiB and the minimum is 64 MiB. Kinesis Data Firehose uses this value for padding calculations.
     * 
     */
    @Import(name="blockSizeBytes")
    private @Nullable Output<Integer> blockSizeBytes;

    /**
     * @return The Hadoop Distributed File System (HDFS) block size. This is useful if you intend to copy the data from Amazon S3 to HDFS before querying. The default is 256 MiB and the minimum is 64 MiB. Kinesis Data Firehose uses this value for padding calculations.
     * 
     */
    public Optional<Output<Integer>> blockSizeBytes() {
        return Optional.ofNullable(this.blockSizeBytes);
    }

    /**
     * The compression code to use over data blocks. The possible values are `UNCOMPRESSED`, `SNAPPY`, and `GZIP`, with the default being `SNAPPY`. Use `SNAPPY` for higher decompression speed. Use `GZIP` if the compression ratio is more important than speed.
     * 
     */
    @Import(name="compression")
    private @Nullable Output<String> compression;

    /**
     * @return The compression code to use over data blocks. The possible values are `UNCOMPRESSED`, `SNAPPY`, and `GZIP`, with the default being `SNAPPY`. Use `SNAPPY` for higher decompression speed. Use `GZIP` if the compression ratio is more important than speed.
     * 
     */
    public Optional<Output<String>> compression() {
        return Optional.ofNullable(this.compression);
    }

    /**
     * Indicates whether to enable dictionary compression.
     * 
     */
    @Import(name="enableDictionaryCompression")
    private @Nullable Output<Boolean> enableDictionaryCompression;

    /**
     * @return Indicates whether to enable dictionary compression.
     * 
     */
    public Optional<Output<Boolean>> enableDictionaryCompression() {
        return Optional.ofNullable(this.enableDictionaryCompression);
    }

    /**
     * The maximum amount of padding to apply. This is useful if you intend to copy the data from Amazon S3 to HDFS before querying. The default is `0`.
     * 
     */
    @Import(name="maxPaddingBytes")
    private @Nullable Output<Integer> maxPaddingBytes;

    /**
     * @return The maximum amount of padding to apply. This is useful if you intend to copy the data from Amazon S3 to HDFS before querying. The default is `0`.
     * 
     */
    public Optional<Output<Integer>> maxPaddingBytes() {
        return Optional.ofNullable(this.maxPaddingBytes);
    }

    /**
     * The Parquet page size. Column chunks are divided into pages. A page is conceptually an indivisible unit (in terms of compression and encoding). The minimum value is 64 KiB and the default is 1 MiB.
     * 
     */
    @Import(name="pageSizeBytes")
    private @Nullable Output<Integer> pageSizeBytes;

    /**
     * @return The Parquet page size. Column chunks are divided into pages. A page is conceptually an indivisible unit (in terms of compression and encoding). The minimum value is 64 KiB and the default is 1 MiB.
     * 
     */
    public Optional<Output<Integer>> pageSizeBytes() {
        return Optional.ofNullable(this.pageSizeBytes);
    }

    /**
     * Indicates the version of row format to output. The possible values are `V1` and `V2`. The default is `V1`.
     * 
     */
    @Import(name="writerVersion")
    private @Nullable Output<String> writerVersion;

    /**
     * @return Indicates the version of row format to output. The possible values are `V1` and `V2`. The default is `V1`.
     * 
     */
    public Optional<Output<String>> writerVersion() {
        return Optional.ofNullable(this.writerVersion);
    }

    private FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs() {}

    private FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs(FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs $) {
        this.blockSizeBytes = $.blockSizeBytes;
        this.compression = $.compression;
        this.enableDictionaryCompression = $.enableDictionaryCompression;
        this.maxPaddingBytes = $.maxPaddingBytes;
        this.pageSizeBytes = $.pageSizeBytes;
        this.writerVersion = $.writerVersion;
    }

    public static Builder builder() {
        return new Builder();
    }
    public static Builder builder(FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs $;

        public Builder() {
            $ = new FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs();
        }

        public Builder(FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs defaults) {
            $ = new FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs(Objects.requireNonNull(defaults));
        }

        /**
         * @param blockSizeBytes The Hadoop Distributed File System (HDFS) block size. This is useful if you intend to copy the data from Amazon S3 to HDFS before querying. The default is 256 MiB and the minimum is 64 MiB. Kinesis Data Firehose uses this value for padding calculations.
         * 
         * @return builder
         * 
         */
        public Builder blockSizeBytes(@Nullable Output<Integer> blockSizeBytes) {
            $.blockSizeBytes = blockSizeBytes;
            return this;
        }

        /**
         * @param blockSizeBytes The Hadoop Distributed File System (HDFS) block size. This is useful if you intend to copy the data from Amazon S3 to HDFS before querying. The default is 256 MiB and the minimum is 64 MiB. Kinesis Data Firehose uses this value for padding calculations.
         * 
         * @return builder
         * 
         */
        public Builder blockSizeBytes(Integer blockSizeBytes) {
            return blockSizeBytes(Output.of(blockSizeBytes));
        }

        /**
         * @param compression The compression code to use over data blocks. The possible values are `UNCOMPRESSED`, `SNAPPY`, and `GZIP`, with the default being `SNAPPY`. Use `SNAPPY` for higher decompression speed. Use `GZIP` if the compression ratio is more important than speed.
         * 
         * @return builder
         * 
         */
        public Builder compression(@Nullable Output<String> compression) {
            $.compression = compression;
            return this;
        }

        /**
         * @param compression The compression code to use over data blocks. The possible values are `UNCOMPRESSED`, `SNAPPY`, and `GZIP`, with the default being `SNAPPY`. Use `SNAPPY` for higher decompression speed. Use `GZIP` if the compression ratio is more important than speed.
         * 
         * @return builder
         * 
         */
        public Builder compression(String compression) {
            return compression(Output.of(compression));
        }

        /**
         * @param enableDictionaryCompression Indicates whether to enable dictionary compression.
         * 
         * @return builder
         * 
         */
        public Builder enableDictionaryCompression(@Nullable Output<Boolean> enableDictionaryCompression) {
            $.enableDictionaryCompression = enableDictionaryCompression;
            return this;
        }

        /**
         * @param enableDictionaryCompression Indicates whether to enable dictionary compression.
         * 
         * @return builder
         * 
         */
        public Builder enableDictionaryCompression(Boolean enableDictionaryCompression) {
            return enableDictionaryCompression(Output.of(enableDictionaryCompression));
        }

        /**
         * @param maxPaddingBytes The maximum amount of padding to apply. This is useful if you intend to copy the data from Amazon S3 to HDFS before querying. The default is `0`.
         * 
         * @return builder
         * 
         */
        public Builder maxPaddingBytes(@Nullable Output<Integer> maxPaddingBytes) {
            $.maxPaddingBytes = maxPaddingBytes;
            return this;
        }

        /**
         * @param maxPaddingBytes The maximum amount of padding to apply. This is useful if you intend to copy the data from Amazon S3 to HDFS before querying. The default is `0`.
         * 
         * @return builder
         * 
         */
        public Builder maxPaddingBytes(Integer maxPaddingBytes) {
            return maxPaddingBytes(Output.of(maxPaddingBytes));
        }

        /**
         * @param pageSizeBytes The Parquet page size. Column chunks are divided into pages. A page is conceptually an indivisible unit (in terms of compression and encoding). The minimum value is 64 KiB and the default is 1 MiB.
         * 
         * @return builder
         * 
         */
        public Builder pageSizeBytes(@Nullable Output<Integer> pageSizeBytes) {
            $.pageSizeBytes = pageSizeBytes;
            return this;
        }

        /**
         * @param pageSizeBytes The Parquet page size. Column chunks are divided into pages. A page is conceptually an indivisible unit (in terms of compression and encoding). The minimum value is 64 KiB and the default is 1 MiB.
         * 
         * @return builder
         * 
         */
        public Builder pageSizeBytes(Integer pageSizeBytes) {
            return pageSizeBytes(Output.of(pageSizeBytes));
        }

        /**
         * @param writerVersion Indicates the version of row format to output. The possible values are `V1` and `V2`. The default is `V1`.
         * 
         * @return builder
         * 
         */
        public Builder writerVersion(@Nullable Output<String> writerVersion) {
            $.writerVersion = writerVersion;
            return this;
        }

        /**
         * @param writerVersion Indicates the version of row format to output. The possible values are `V1` and `V2`. The default is `V1`.
         * 
         * @return builder
         * 
         */
        public Builder writerVersion(String writerVersion) {
            return writerVersion(Output.of(writerVersion));
        }

        public FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs build() {
            return $;
        }
    }

}
