// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.aws.sagemaker.outputs;

import com.pulumi.aws.sagemaker.outputs.DataQualityJobDefinitionDataQualityJobInputBatchTransformInputDatasetFormat;
import com.pulumi.core.annotations.CustomType;
import java.lang.String;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;

@CustomType
public final class DataQualityJobDefinitionDataQualityJobInputBatchTransformInput {
    /**
     * @return The Amazon S3 location being used to capture the data.
     * 
     */
    private String dataCapturedDestinationS3Uri;
    /**
     * @return The dataset format for your batch transform job. Fields are documented below.
     * 
     */
    private DataQualityJobDefinitionDataQualityJobInputBatchTransformInputDatasetFormat datasetFormat;
    /**
     * @return Path to the filesystem where the batch transform data is available to the container. Defaults to `/opt/ml/processing/input`.
     * 
     */
    private @Nullable String localPath;
    /**
     * @return Whether input data distributed in Amazon S3 is fully replicated or sharded by an S3 key. Defaults to `FullyReplicated`. Valid values are `FullyReplicated` or `ShardedByS3Key`
     * 
     */
    private @Nullable String s3DataDistributionType;
    /**
     * @return Whether the `Pipe` or `File` is used as the input mode for transferring data for the monitoring job. `Pipe` mode is recommended for large datasets. `File` mode is useful for small files that fit in memory. Defaults to `File`.  Valid values are `Pipe` or `File`
     * 
     */
    private @Nullable String s3InputMode;

    private DataQualityJobDefinitionDataQualityJobInputBatchTransformInput() {}
    /**
     * @return The Amazon S3 location being used to capture the data.
     * 
     */
    public String dataCapturedDestinationS3Uri() {
        return this.dataCapturedDestinationS3Uri;
    }
    /**
     * @return The dataset format for your batch transform job. Fields are documented below.
     * 
     */
    public DataQualityJobDefinitionDataQualityJobInputBatchTransformInputDatasetFormat datasetFormat() {
        return this.datasetFormat;
    }
    /**
     * @return Path to the filesystem where the batch transform data is available to the container. Defaults to `/opt/ml/processing/input`.
     * 
     */
    public Optional<String> localPath() {
        return Optional.ofNullable(this.localPath);
    }
    /**
     * @return Whether input data distributed in Amazon S3 is fully replicated or sharded by an S3 key. Defaults to `FullyReplicated`. Valid values are `FullyReplicated` or `ShardedByS3Key`
     * 
     */
    public Optional<String> s3DataDistributionType() {
        return Optional.ofNullable(this.s3DataDistributionType);
    }
    /**
     * @return Whether the `Pipe` or `File` is used as the input mode for transferring data for the monitoring job. `Pipe` mode is recommended for large datasets. `File` mode is useful for small files that fit in memory. Defaults to `File`.  Valid values are `Pipe` or `File`
     * 
     */
    public Optional<String> s3InputMode() {
        return Optional.ofNullable(this.s3InputMode);
    }

    public static Builder builder() {
        return new Builder();
    }

    public static Builder builder(DataQualityJobDefinitionDataQualityJobInputBatchTransformInput defaults) {
        return new Builder(defaults);
    }
    @CustomType.Builder
    public static final class Builder {
        private String dataCapturedDestinationS3Uri;
        private DataQualityJobDefinitionDataQualityJobInputBatchTransformInputDatasetFormat datasetFormat;
        private @Nullable String localPath;
        private @Nullable String s3DataDistributionType;
        private @Nullable String s3InputMode;
        public Builder() {}
        public Builder(DataQualityJobDefinitionDataQualityJobInputBatchTransformInput defaults) {
    	      Objects.requireNonNull(defaults);
    	      this.dataCapturedDestinationS3Uri = defaults.dataCapturedDestinationS3Uri;
    	      this.datasetFormat = defaults.datasetFormat;
    	      this.localPath = defaults.localPath;
    	      this.s3DataDistributionType = defaults.s3DataDistributionType;
    	      this.s3InputMode = defaults.s3InputMode;
        }

        @CustomType.Setter
        public Builder dataCapturedDestinationS3Uri(String dataCapturedDestinationS3Uri) {
            this.dataCapturedDestinationS3Uri = Objects.requireNonNull(dataCapturedDestinationS3Uri);
            return this;
        }
        @CustomType.Setter
        public Builder datasetFormat(DataQualityJobDefinitionDataQualityJobInputBatchTransformInputDatasetFormat datasetFormat) {
            this.datasetFormat = Objects.requireNonNull(datasetFormat);
            return this;
        }
        @CustomType.Setter
        public Builder localPath(@Nullable String localPath) {
            this.localPath = localPath;
            return this;
        }
        @CustomType.Setter
        public Builder s3DataDistributionType(@Nullable String s3DataDistributionType) {
            this.s3DataDistributionType = s3DataDistributionType;
            return this;
        }
        @CustomType.Setter
        public Builder s3InputMode(@Nullable String s3InputMode) {
            this.s3InputMode = s3InputMode;
            return this;
        }
        public DataQualityJobDefinitionDataQualityJobInputBatchTransformInput build() {
            final var _resultValue = new DataQualityJobDefinitionDataQualityJobInputBatchTransformInput();
            _resultValue.dataCapturedDestinationS3Uri = dataCapturedDestinationS3Uri;
            _resultValue.datasetFormat = datasetFormat;
            _resultValue.localPath = localPath;
            _resultValue.s3DataDistributionType = s3DataDistributionType;
            _resultValue.s3InputMode = s3InputMode;
            return _resultValue;
        }
    }
}
