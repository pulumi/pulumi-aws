// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../../types/input";
import * as outputs from "../../types/output";
import * as enums from "../../types/enums";
import * as utilities from "../../utilities";

import {RoutingRule} from "@/s3";

export interface ConnectorProfileConnectorProfileConfig {
    /**
     * The connector-specific credentials required by each connector. See Connector Profile Credentials for more details.
     */
    connectorProfileCredentials: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentials;
    /**
     * The connector-specific properties of the profile configuration. See Connector Profile Properties for more details.
     */
    connectorProfileProperties: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileProperties;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentials {
    /**
     * The connector-specific credentials required when using Amplitude. See Amplitude Connector Profile Credentials for more details.
     */
    amplitude?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsAmplitude;
    /**
     * The connector-specific profile properties required when using the custom connector. See Custom Connector Profile Properties for more details.
     */
    customConnector?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsCustomConnector;
    /**
     * Connector-specific properties required when using Datadog. See Generic Connector Profile Properties for more details.
     */
    datadog?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsDatadog;
    /**
     * The connector-specific properties required when using Dynatrace. See Generic Connector Profile Properties for more details.
     */
    dynatrace?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsDynatrace;
    /**
     * The connector-specific credentials required when using Google Analytics. See Google Analytics Connector Profile Credentials for more details.
     */
    googleAnalytics?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsGoogleAnalytics;
    /**
     * The connector-specific credentials required when using Amazon Honeycode. See Honeycode Connector Profile Credentials for more details.
     */
    honeycode?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsHoneycode;
    /**
     * The connector-specific properties required when using Infor Nexus. See Generic Connector Profile Properties for more details.
     */
    inforNexus?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsInforNexus;
    /**
     * Connector-specific properties required when using Marketo. See Generic Connector Profile Properties for more details.
     */
    marketo?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsMarketo;
    /**
     * Connector-specific properties required when using Amazon Redshift. See Redshift Connector Profile Properties for more details.
     */
    redshift?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsRedshift;
    /**
     * The connector-specific properties required when using Salesforce. See Salesforce Connector Profile Properties for more details.
     */
    salesforce?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSalesforce;
    /**
     * The connector-specific properties required when using SAPOData. See SAPOData Connector Profile Properties for more details.
     */
    sapoData?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSapoData;
    /**
     * The connector-specific properties required when using ServiceNow. See Generic Connector Profile Properties for more details.
     */
    serviceNow?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsServiceNow;
    /**
     * Connector-specific credentials required when using Singular. See Singular Connector Profile Credentials for more details.
     */
    singular?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSingular;
    /**
     * Connector-specific properties required when using Slack. See Generic Connector Profile Properties for more details.
     */
    slack?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSlack;
    /**
     * The connector-specific properties required when using Snowflake. See Snowflake Connector Profile Properties for more details.
     */
    snowflake?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSnowflake;
    /**
     * The connector-specific credentials required when using Trend Micro. See Trend Micro Connector Profile Credentials for more details.
     */
    trendmicro?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsTrendmicro;
    /**
     * Connector-specific properties required when using Veeva. See Generic Connector Profile Properties for more details.
     */
    veeva?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsVeeva;
    /**
     * Connector-specific properties required when using Zendesk. See Generic Connector Profile Properties for more details.
     */
    zendesk?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsZendesk;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsAmplitude {
    /**
     * Unique alphanumeric identifier used to authenticate a user, developer, or calling program to your API.
     */
    apiKey: string;
    /**
     * The Secret Access Key portion of the credentials.
     */
    secretKey: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsCustomConnector {
    /**
     * Unique alphanumeric identifier used to authenticate a user, developer, or calling program to your API.
     */
    apiKey?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsCustomConnectorApiKey;
    /**
     * The authentication type that the custom connector uses for authenticating while creating a connector profile. One of: `APIKEY`, `BASIC`, `CUSTOM`, `OAUTH2`.
     */
    authenticationType: string;
    /**
     * Basic credentials that are required for the authentication of the user.
     */
    basic?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsCustomConnectorBasic;
    /**
     * If the connector uses the custom authentication mechanism, this holds the required credentials.
     */
    custom?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsCustomConnectorCustom;
    /**
     * OAuth 2.0 credentials required for the authentication of the user.
     */
    oauth2?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsCustomConnectorOauth2;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsCustomConnectorApiKey {
    /**
     * Unique alphanumeric identifier used to authenticate a user, developer, or calling program to your API.
     */
    apiKey: string;
    /**
     * The Secret Access Key portion of the credentials.
     */
    apiSecretKey?: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsCustomConnectorBasic {
    /**
     * Password that corresponds to the user name.
     */
    password: string;
    /**
     * Name of the user.
     */
    username: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsCustomConnectorCustom {
    /**
     * A map that holds custom authentication credentials.
     */
    credentialsMap?: {[key: string]: string};
    /**
     * The custom authentication type that the connector uses.
     */
    customAuthenticationType: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsCustomConnectorOauth2 {
    /**
     * The credentials used to access protected Zendesk resources.
     */
    accessToken?: string;
    /**
     * The identifier for the desired client.
     */
    clientId?: string;
    /**
     * The client secret used by the OAuth client to authenticate to the authorization server.
     */
    clientSecret?: string;
    /**
     * The OAuth requirement needed to request security tokens from the connector endpoint. See OAuth Request for more details.
     */
    oauthRequest?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsCustomConnectorOauth2OauthRequest;
    /**
     * The refresh token used to refresh expired access token.
     */
    refreshToken?: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsCustomConnectorOauth2OauthRequest {
    /**
     * The code provided by the connector when it has been authenticated via the connected app.
     */
    authCode?: string;
    /**
     * The URL to which the authentication server redirects the browser after authorization has been granted.
     */
    redirectUri?: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsDatadog {
    /**
     * Unique alphanumeric identifier used to authenticate a user, developer, or calling program to your API.
     */
    apiKey: string;
    /**
     * Application keys, in conjunction with your API key, give you full access to Datadogâ€™s programmatic API. Application keys are associated with the user account that created them. The application key is used to log all requests made to the API.
     */
    applicationKey: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsDynatrace {
    /**
     * The API tokens used by Dynatrace API to authenticate various API calls.
     */
    apiToken: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsGoogleAnalytics {
    /**
     * The credentials used to access protected Zendesk resources.
     */
    accessToken?: string;
    /**
     * The identifier for the desired client.
     */
    clientId: string;
    /**
     * The client secret used by the OAuth client to authenticate to the authorization server.
     */
    clientSecret: string;
    /**
     * The OAuth requirement needed to request security tokens from the connector endpoint. See OAuth Request for more details.
     */
    oauthRequest?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsGoogleAnalyticsOauthRequest;
    /**
     * The refresh token used to refresh expired access token.
     */
    refreshToken?: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsGoogleAnalyticsOauthRequest {
    /**
     * The code provided by the connector when it has been authenticated via the connected app.
     */
    authCode?: string;
    /**
     * The URL to which the authentication server redirects the browser after authorization has been granted.
     */
    redirectUri?: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsHoneycode {
    /**
     * The credentials used to access protected Zendesk resources.
     */
    accessToken?: string;
    /**
     * The OAuth requirement needed to request security tokens from the connector endpoint. See OAuth Request for more details.
     */
    oauthRequest?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsHoneycodeOauthRequest;
    /**
     * The refresh token used to refresh expired access token.
     */
    refreshToken?: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsHoneycodeOauthRequest {
    /**
     * The code provided by the connector when it has been authenticated via the connected app.
     */
    authCode?: string;
    /**
     * The URL to which the authentication server redirects the browser after authorization has been granted.
     */
    redirectUri?: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsInforNexus {
    /**
     * The Access Key portion of the credentials.
     */
    accessKeyId: string;
    /**
     * Encryption keys used to encrypt data.
     */
    datakey: string;
    /**
     * The secret key used to sign requests.
     */
    secretAccessKey: string;
    /**
     * Identifier for the user.
     */
    userId: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsMarketo {
    /**
     * The credentials used to access protected Zendesk resources.
     */
    accessToken?: string;
    /**
     * The identifier for the desired client.
     */
    clientId: string;
    /**
     * The client secret used by the OAuth client to authenticate to the authorization server.
     */
    clientSecret: string;
    /**
     * The OAuth requirement needed to request security tokens from the connector endpoint. See OAuth Request for more details.
     */
    oauthRequest?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsMarketoOauthRequest;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsMarketoOauthRequest {
    /**
     * The code provided by the connector when it has been authenticated via the connected app.
     */
    authCode?: string;
    /**
     * The URL to which the authentication server redirects the browser after authorization has been granted.
     */
    redirectUri?: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsRedshift {
    /**
     * Password that corresponds to the user name.
     */
    password: string;
    /**
     * Name of the user.
     */
    username: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSalesforce {
    /**
     * The credentials used to access protected Zendesk resources.
     */
    accessToken?: string;
    /**
     * The secret manager ARN, which contains the client ID and client secret of the connected app.
     */
    clientCredentialsArn?: string;
    /**
     * The OAuth requirement needed to request security tokens from the connector endpoint. See OAuth Request for more details.
     */
    oauthRequest?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSalesforceOauthRequest;
    /**
     * The refresh token used to refresh expired access token.
     */
    refreshToken?: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSalesforceOauthRequest {
    /**
     * The code provided by the connector when it has been authenticated via the connected app.
     */
    authCode?: string;
    /**
     * The URL to which the authentication server redirects the browser after authorization has been granted.
     */
    redirectUri?: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSapoData {
    /**
     * The SAPOData basic authentication credentials.
     */
    basicAuthCredentials?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSapoDataBasicAuthCredentials;
    /**
     * The SAPOData OAuth type authentication credentials.
     */
    oauthCredentials?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSapoDataOauthCredentials;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSapoDataBasicAuthCredentials {
    /**
     * Password that corresponds to the user name.
     */
    password: string;
    /**
     * Name of the user.
     */
    username: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSapoDataOauthCredentials {
    /**
     * The credentials used to access protected Zendesk resources.
     */
    accessToken?: string;
    /**
     * The identifier for the desired client.
     */
    clientId: string;
    /**
     * The client secret used by the OAuth client to authenticate to the authorization server.
     */
    clientSecret: string;
    /**
     * The OAuth requirement needed to request security tokens from the connector endpoint. See OAuth Request for more details.
     */
    oauthRequest?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSapoDataOauthCredentialsOauthRequest;
    /**
     * The refresh token used to refresh expired access token.
     */
    refreshToken?: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSapoDataOauthCredentialsOauthRequest {
    /**
     * The code provided by the connector when it has been authenticated via the connected app.
     */
    authCode?: string;
    /**
     * The URL to which the authentication server redirects the browser after authorization has been granted.
     */
    redirectUri?: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsServiceNow {
    /**
     * Password that corresponds to the user name.
     */
    password: string;
    /**
     * Name of the user.
     */
    username: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSingular {
    /**
     * Unique alphanumeric identifier used to authenticate a user, developer, or calling program to your API.
     */
    apiKey: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSlack {
    /**
     * The credentials used to access protected Zendesk resources.
     */
    accessToken?: string;
    /**
     * The identifier for the desired client.
     */
    clientId: string;
    /**
     * The client secret used by the OAuth client to authenticate to the authorization server.
     */
    clientSecret: string;
    /**
     * The OAuth requirement needed to request security tokens from the connector endpoint. See OAuth Request for more details.
     */
    oauthRequest?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSlackOauthRequest;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSlackOauthRequest {
    /**
     * The code provided by the connector when it has been authenticated via the connected app.
     */
    authCode?: string;
    /**
     * The URL to which the authentication server redirects the browser after authorization has been granted.
     */
    redirectUri?: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsSnowflake {
    /**
     * Password that corresponds to the user name.
     */
    password: string;
    /**
     * Name of the user.
     */
    username: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsTrendmicro {
    /**
     * The Secret Access Key portion of the credentials.
     */
    apiSecretKey: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsVeeva {
    /**
     * Password that corresponds to the user name.
     */
    password: string;
    /**
     * Name of the user.
     */
    username: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsZendesk {
    /**
     * The credentials used to access protected Zendesk resources.
     */
    accessToken?: string;
    /**
     * The identifier for the desired client.
     */
    clientId: string;
    /**
     * The client secret used by the OAuth client to authenticate to the authorization server.
     */
    clientSecret: string;
    /**
     * The OAuth requirement needed to request security tokens from the connector endpoint. See OAuth Request for more details.
     */
    oauthRequest?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsZendeskOauthRequest;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileCredentialsZendeskOauthRequest {
    /**
     * The code provided by the connector when it has been authenticated via the connected app.
     */
    authCode?: string;
    /**
     * The URL to which the authentication server redirects the browser after authorization has been granted.
     */
    redirectUri?: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfileProperties {
    /**
     * The connector-specific credentials required when using Amplitude. See Amplitude Connector Profile Credentials for more details.
     */
    amplitude?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesAmplitude;
    /**
     * The connector-specific profile properties required when using the custom connector. See Custom Connector Profile Properties for more details.
     */
    customConnector?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesCustomConnector;
    /**
     * Connector-specific properties required when using Datadog. See Generic Connector Profile Properties for more details.
     */
    datadog?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesDatadog;
    /**
     * The connector-specific properties required when using Dynatrace. See Generic Connector Profile Properties for more details.
     */
    dynatrace?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesDynatrace;
    /**
     * The connector-specific credentials required when using Google Analytics. See Google Analytics Connector Profile Credentials for more details.
     */
    googleAnalytics?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesGoogleAnalytics;
    /**
     * The connector-specific credentials required when using Amazon Honeycode. See Honeycode Connector Profile Credentials for more details.
     */
    honeycode?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesHoneycode;
    /**
     * The connector-specific properties required when using Infor Nexus. See Generic Connector Profile Properties for more details.
     */
    inforNexus?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesInforNexus;
    /**
     * Connector-specific properties required when using Marketo. See Generic Connector Profile Properties for more details.
     */
    marketo?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesMarketo;
    /**
     * Connector-specific properties required when using Amazon Redshift. See Redshift Connector Profile Properties for more details.
     */
    redshift?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesRedshift;
    /**
     * The connector-specific properties required when using Salesforce. See Salesforce Connector Profile Properties for more details.
     */
    salesforce?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesSalesforce;
    /**
     * The connector-specific properties required when using SAPOData. See SAPOData Connector Profile Properties for more details.
     */
    sapoData?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesSapoData;
    /**
     * The connector-specific properties required when using ServiceNow. See Generic Connector Profile Properties for more details.
     */
    serviceNow?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesServiceNow;
    /**
     * Connector-specific credentials required when using Singular. See Singular Connector Profile Credentials for more details.
     */
    singular?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesSingular;
    /**
     * Connector-specific properties required when using Slack. See Generic Connector Profile Properties for more details.
     */
    slack?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesSlack;
    /**
     * The connector-specific properties required when using Snowflake. See Snowflake Connector Profile Properties for more details.
     */
    snowflake?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesSnowflake;
    /**
     * The connector-specific credentials required when using Trend Micro. See Trend Micro Connector Profile Credentials for more details.
     */
    trendmicro?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesTrendmicro;
    /**
     * Connector-specific properties required when using Veeva. See Generic Connector Profile Properties for more details.
     */
    veeva?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesVeeva;
    /**
     * Connector-specific properties required when using Zendesk. See Generic Connector Profile Properties for more details.
     */
    zendesk?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesZendesk;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesAmplitude {
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesCustomConnector {
    /**
     * The OAuth 2.0 properties required for OAuth 2.0 authentication.
     */
    oauth2Properties?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesCustomConnectorOauth2Properties;
    /**
     * A map of properties that are required to create a profile for the custom connector.
     */
    profileProperties?: {[key: string]: string};
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesCustomConnectorOauth2Properties {
    /**
     * The OAuth 2.0 grant type used by connector for OAuth 2.0 authentication. One of: `AUTHORIZATION_CODE`, `CLIENT_CREDENTIALS`.
     */
    oauth2GrantType: string;
    /**
     * The token url required to fetch access/refresh tokens using authorization code and also to refresh expired access token using refresh token.
     */
    tokenUrl: string;
    /**
     * Associates your token URL with a map of properties that you define. Use this parameter to provide any additional details that the connector requires to authenticate your request.
     */
    tokenUrlCustomProperties?: {[key: string]: string};
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesDatadog {
    /**
     * The location of the Salesforce resource.
     */
    instanceUrl: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesDynatrace {
    /**
     * The location of the Salesforce resource.
     */
    instanceUrl: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesGoogleAnalytics {
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesHoneycode {
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesInforNexus {
    /**
     * The location of the Salesforce resource.
     */
    instanceUrl: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesMarketo {
    /**
     * The location of the Salesforce resource.
     */
    instanceUrl: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesRedshift {
    /**
     * The name of the Amazon S3 bucket associated with Snowflake.
     */
    bucketName: string;
    /**
     * The bucket path that refers to the Amazon S3 bucket associated with Snowflake.
     */
    bucketPrefix?: string;
    /**
     * The JDBC URL of the Amazon Redshift cluster.
     */
    databaseUrl?: string;
    /**
     * ARN of the IAM role.
     */
    roleArn: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesSalesforce {
    /**
     * The location of the Salesforce resource.
     */
    instanceUrl?: string;
    /**
     * Indicates whether the connector profile applies to a sandbox or production environment.
     */
    isSandboxEnvironment?: boolean;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesSapoData {
    /**
     * The location of the SAPOData resource.
     */
    applicationHostUrl: string;
    /**
     * The application path to catalog service.
     */
    applicationServicePath: string;
    /**
     * The client number for the client creating the connection.
     */
    clientNumber: string;
    /**
     * The logon language of SAPOData instance.
     */
    logonLanguage?: string;
    /**
     * The SAPOData OAuth properties required for OAuth type authentication.
     */
    oauthProperties?: outputs.appflow.ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesSapoDataOauthProperties;
    /**
     * The port number of the SAPOData instance.
     */
    portNumber: number;
    /**
     * The Snowflake Private Link service name to be used for private data transfers.
     */
    privateLinkServiceName?: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesSapoDataOauthProperties {
    /**
     * The authorization code url required to redirect to SAP Login Page to fetch authorization code for OAuth type authentication.
     */
    authCodeUrl: string;
    /**
     * The OAuth scopes required for OAuth type authentication.
     */
    oauthScopes: string[];
    /**
     * The token url required to fetch access/refresh tokens using authorization code and also to refresh expired access token using refresh token.
     */
    tokenUrl: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesServiceNow {
    /**
     * The location of the Salesforce resource.
     */
    instanceUrl: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesSingular {
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesSlack {
    /**
     * The location of the Salesforce resource.
     */
    instanceUrl: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesSnowflake {
    /**
     * The name of the account.
     */
    accountName?: string;
    /**
     * The name of the Amazon S3 bucket associated with Snowflake.
     */
    bucketName: string;
    /**
     * The bucket path that refers to the Amazon S3 bucket associated with Snowflake.
     */
    bucketPrefix?: string;
    /**
     * The Snowflake Private Link service name to be used for private data transfers.
     */
    privateLinkServiceName?: string;
    /**
     * AWS Region of the Snowflake account.
     */
    region?: string;
    /**
     * Name of the Amazon S3 stage that was created while setting up an Amazon S3 stage in the Snowflake account. This is written in the following format: `<Database>.<Schema>.<Stage Name>`.
     */
    stage: string;
    /**
     * The name of the Snowflake warehouse.
     */
    warehouse: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesTrendmicro {
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesVeeva {
    /**
     * The location of the Salesforce resource.
     */
    instanceUrl: string;
}

export interface ConnectorProfileConnectorProfileConfigConnectorProfilePropertiesZendesk {
    /**
     * The location of the Salesforce resource.
     */
    instanceUrl: string;
}

export interface FlowDestinationFlowConfig {
    /**
     * API version that the destination connector uses.
     */
    apiVersion?: string;
    /**
     * Name of the connector profile. This name must be unique for each connector profile in the AWS account.
     */
    connectorProfileName?: string;
    /**
     * Type of connector, such as Salesforce, Amplitude, and so on. Valid values are `Salesforce`, `Singular`, `Slack`, `Redshift`, `S3`, `Marketo`, `Googleanalytics`, `Zendesk`, `Servicenow`, `Datadog`, `Trendmicro`, `Snowflake`, `Dynatrace`, `Infornexus`, `Amplitude`, `Veeva`, `EventBridge`, `LookoutMetrics`, `Upsolver`, `Honeycode`, `CustomerProfiles`, `SAPOData`, and `CustomConnector`.
     */
    connectorType: string;
    /**
     * This stores the information that is required to query a particular connector. See Destination Connector Properties for more information.
     */
    destinationConnectorProperties: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorProperties;
}

export interface FlowDestinationFlowConfigDestinationConnectorProperties {
    /**
     * Operators supported by the custom connector. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    customConnector?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesCustomConnector;
    /**
     * Properties that are required to query Amazon Connect Customer Profiles. See Customer Profiles Destination Properties for more details.
     */
    customerProfiles?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesCustomerProfiles;
    /**
     * Properties that are required to query Amazon EventBridge. See Generic Destination Properties for more details.
     */
    eventBridge?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesEventBridge;
    /**
     * Properties that are required to query Amazon Honeycode. See Generic Destination Properties for more details.
     */
    honeycode?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesHoneycode;
    lookoutMetrics?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesLookoutMetrics;
    /**
     * Operation to be performed on the provided Marketo source fields. Valid values are `PROJECTION`, `BETWEEN`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    marketo?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesMarketo;
    /**
     * Properties that are required to query Amazon Redshift. See Redshift Destination Properties for more details.
     */
    redshift?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesRedshift;
    /**
     * Operation to be performed on the provided Amazon S3 source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    s3?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesS3;
    /**
     * Operation to be performed on the provided Salesforce source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    salesforce?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesSalesforce;
    /**
     * Operation to be performed on the provided SAPOData source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    sapoData?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesSapoData;
    /**
     * Properties that are required to query Snowflake. See Snowflake Destination Properties for more details.
     */
    snowflake?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesSnowflake;
    /**
     * Properties that are required to query Upsolver. See Upsolver Destination Properties for more details.
     */
    upsolver?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesUpsolver;
    /**
     * Operation to be performed on the provided Zendesk source fields. Valid values are `PROJECTION`, `GREATER_THAN`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    zendesk?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesZendesk;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesCustomConnector {
    /**
     * Custom properties that are specific to the connector when it's used as a source in the flow. Maximum of 50 items.
     */
    customProperties?: {[key: string]: string};
    /**
     * Entity specified in the custom connector as a source in the flow.
     */
    entityName: string;
    /**
     * Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
     */
    errorHandlingConfig?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesCustomConnectorErrorHandlingConfig;
    /**
     * Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update or delete.
     */
    idFieldNames?: string[];
    /**
     * This specifies the type of write operation to be performed in Zendesk. When the value is `UPSERT`, then `idFieldNames` is required. Valid values are `INSERT`, `UPSERT`, `UPDATE`, and `DELETE`.
     */
    writeOperationType?: string;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesCustomConnectorErrorHandlingConfig {
    /**
     * Amazon S3 bucket name where the source files are stored.
     */
    bucketName?: string;
    /**
     * Object key for the Amazon S3 bucket in which the source files are stored.
     */
    bucketPrefix?: string;
    /**
     * If the flow should fail after the first instance of a failure when attempting to place data in the destination.
     */
    failOnFirstDestinationError?: boolean;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesCustomerProfiles {
    /**
     * Unique name of the Amazon Connect Customer Profiles domain.
     */
    domainName: string;
    /**
     * Object specified in the Amazon Connect Customer Profiles flow destination.
     */
    objectTypeName?: string;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesEventBridge {
    /**
     * Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
     */
    errorHandlingConfig?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesEventBridgeErrorHandlingConfig;
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesEventBridgeErrorHandlingConfig {
    /**
     * Amazon S3 bucket name where the source files are stored.
     */
    bucketName?: string;
    /**
     * Object key for the Amazon S3 bucket in which the source files are stored.
     */
    bucketPrefix?: string;
    /**
     * If the flow should fail after the first instance of a failure when attempting to place data in the destination.
     */
    failOnFirstDestinationError?: boolean;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesHoneycode {
    /**
     * Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
     */
    errorHandlingConfig?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesHoneycodeErrorHandlingConfig;
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesHoneycodeErrorHandlingConfig {
    /**
     * Amazon S3 bucket name where the source files are stored.
     */
    bucketName?: string;
    /**
     * Object key for the Amazon S3 bucket in which the source files are stored.
     */
    bucketPrefix?: string;
    /**
     * If the flow should fail after the first instance of a failure when attempting to place data in the destination.
     */
    failOnFirstDestinationError?: boolean;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesLookoutMetrics {
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesMarketo {
    /**
     * Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
     */
    errorHandlingConfig?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesMarketoErrorHandlingConfig;
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesMarketoErrorHandlingConfig {
    /**
     * Amazon S3 bucket name where the source files are stored.
     */
    bucketName?: string;
    /**
     * Object key for the Amazon S3 bucket in which the source files are stored.
     */
    bucketPrefix?: string;
    /**
     * If the flow should fail after the first instance of a failure when attempting to place data in the destination.
     */
    failOnFirstDestinationError?: boolean;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesRedshift {
    /**
     * Object key for the Amazon S3 bucket in which the source files are stored.
     */
    bucketPrefix?: string;
    /**
     * Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
     */
    errorHandlingConfig?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesRedshiftErrorHandlingConfig;
    /**
     * Intermediate bucket that Amazon AppFlow uses when moving data into Amazon Snowflake.
     */
    intermediateBucketName: string;
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesRedshiftErrorHandlingConfig {
    /**
     * Amazon S3 bucket name where the source files are stored.
     */
    bucketName?: string;
    /**
     * Object key for the Amazon S3 bucket in which the source files are stored.
     */
    bucketPrefix?: string;
    /**
     * If the flow should fail after the first instance of a failure when attempting to place data in the destination.
     */
    failOnFirstDestinationError?: boolean;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesS3 {
    /**
     * Amazon S3 bucket name where the source files are stored.
     */
    bucketName: string;
    /**
     * Object key for the Amazon S3 bucket in which the source files are stored.
     */
    bucketPrefix?: string;
    /**
     * Configuration that determines how Amazon AppFlow should format the flow output data when Upsolver is used as the destination. See Upsolver S3 Output Format Config for more details.
     */
    s3OutputFormatConfig?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfig;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfig {
    /**
     * Aggregation settings that you can use to customize the output format of your flow data. See Aggregation Config for more details.
     */
    aggregationConfig?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfigAggregationConfig;
    /**
     * File type that Amazon AppFlow places in the Upsolver Amazon S3 bucket. Valid values are `CSV`, `JSON`, and `PARQUET`.
     */
    fileType?: string;
    /**
     * Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See Prefix Config for more details.
     */
    prefixConfig?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfigPrefixConfig;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfigAggregationConfig {
    /**
     * Whether Amazon AppFlow aggregates the flow records into a single file, or leave them unaggregated. Valid values are `None` and `SingleFile`.
     */
    aggregationType?: string;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfigPrefixConfig {
    /**
     * Determines the level of granularity that's included in the prefix. Valid values are `YEAR`, `MONTH`, `DAY`, `HOUR`, and `MINUTE`.
     */
    prefixFormat?: string;
    /**
     * Determines the format of the prefix, and whether it applies to the file name, file path, or both. Valid values are `FILENAME`, `PATH`, and `PATH_AND_FILENAME`.
     */
    prefixType?: string;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesSalesforce {
    /**
     * Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
     */
    errorHandlingConfig?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesSalesforceErrorHandlingConfig;
    /**
     * Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update or delete.
     */
    idFieldNames?: string[];
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
    /**
     * This specifies the type of write operation to be performed in Zendesk. When the value is `UPSERT`, then `idFieldNames` is required. Valid values are `INSERT`, `UPSERT`, `UPDATE`, and `DELETE`.
     */
    writeOperationType?: string;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesSalesforceErrorHandlingConfig {
    /**
     * Amazon S3 bucket name where the source files are stored.
     */
    bucketName?: string;
    /**
     * Object key for the Amazon S3 bucket in which the source files are stored.
     */
    bucketPrefix?: string;
    /**
     * If the flow should fail after the first instance of a failure when attempting to place data in the destination.
     */
    failOnFirstDestinationError?: boolean;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesSapoData {
    /**
     * Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
     */
    errorHandlingConfig?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesSapoDataErrorHandlingConfig;
    /**
     * Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update or delete.
     */
    idFieldNames?: string[];
    /**
     * Object path specified in the SAPOData flow source.
     */
    objectPath: string;
    /**
     * Determines how Amazon AppFlow handles the success response that it gets from the connector after placing data. See Success Response Handling Config for more details.
     */
    successResponseHandlingConfig?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesSapoDataSuccessResponseHandlingConfig;
    /**
     * This specifies the type of write operation to be performed in Zendesk. When the value is `UPSERT`, then `idFieldNames` is required. Valid values are `INSERT`, `UPSERT`, `UPDATE`, and `DELETE`.
     */
    writeOperationType?: string;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesSapoDataErrorHandlingConfig {
    /**
     * Amazon S3 bucket name where the source files are stored.
     */
    bucketName?: string;
    /**
     * Object key for the Amazon S3 bucket in which the source files are stored.
     */
    bucketPrefix?: string;
    /**
     * If the flow should fail after the first instance of a failure when attempting to place data in the destination.
     */
    failOnFirstDestinationError?: boolean;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesSapoDataSuccessResponseHandlingConfig {
    /**
     * Amazon S3 bucket name where the source files are stored.
     */
    bucketName?: string;
    /**
     * Object key for the Amazon S3 bucket in which the source files are stored.
     */
    bucketPrefix?: string;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesSnowflake {
    /**
     * Object key for the Amazon S3 bucket in which the source files are stored.
     */
    bucketPrefix?: string;
    /**
     * Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
     */
    errorHandlingConfig?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesSnowflakeErrorHandlingConfig;
    /**
     * Intermediate bucket that Amazon AppFlow uses when moving data into Amazon Snowflake.
     */
    intermediateBucketName: string;
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesSnowflakeErrorHandlingConfig {
    /**
     * Amazon S3 bucket name where the source files are stored.
     */
    bucketName?: string;
    /**
     * Object key for the Amazon S3 bucket in which the source files are stored.
     */
    bucketPrefix?: string;
    /**
     * If the flow should fail after the first instance of a failure when attempting to place data in the destination.
     */
    failOnFirstDestinationError?: boolean;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesUpsolver {
    /**
     * Amazon S3 bucket name where the source files are stored.
     */
    bucketName: string;
    /**
     * Object key for the Amazon S3 bucket in which the source files are stored.
     */
    bucketPrefix?: string;
    /**
     * Configuration that determines how Amazon AppFlow should format the flow output data when Upsolver is used as the destination. See Upsolver S3 Output Format Config for more details.
     */
    s3OutputFormatConfig: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfig;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfig {
    /**
     * Aggregation settings that you can use to customize the output format of your flow data. See Aggregation Config for more details.
     */
    aggregationConfig?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfigAggregationConfig;
    /**
     * File type that Amazon AppFlow places in the Upsolver Amazon S3 bucket. Valid values are `CSV`, `JSON`, and `PARQUET`.
     */
    fileType?: string;
    /**
     * Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See Prefix Config for more details.
     */
    prefixConfig: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfigPrefixConfig;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfigAggregationConfig {
    /**
     * Whether Amazon AppFlow aggregates the flow records into a single file, or leave them unaggregated. Valid values are `None` and `SingleFile`.
     */
    aggregationType?: string;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfigPrefixConfig {
    /**
     * Determines the level of granularity that's included in the prefix. Valid values are `YEAR`, `MONTH`, `DAY`, `HOUR`, and `MINUTE`.
     */
    prefixFormat?: string;
    /**
     * Determines the format of the prefix, and whether it applies to the file name, file path, or both. Valid values are `FILENAME`, `PATH`, and `PATH_AND_FILENAME`.
     */
    prefixType: string;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesZendesk {
    /**
     * Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
     */
    errorHandlingConfig?: outputs.appflow.FlowDestinationFlowConfigDestinationConnectorPropertiesZendeskErrorHandlingConfig;
    /**
     * Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update or delete.
     */
    idFieldNames?: string[];
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
    /**
     * This specifies the type of write operation to be performed in Zendesk. When the value is `UPSERT`, then `idFieldNames` is required. Valid values are `INSERT`, `UPSERT`, `UPDATE`, and `DELETE`.
     */
    writeOperationType?: string;
}

export interface FlowDestinationFlowConfigDestinationConnectorPropertiesZendeskErrorHandlingConfig {
    /**
     * Amazon S3 bucket name where the source files are stored.
     */
    bucketName?: string;
    /**
     * Object key for the Amazon S3 bucket in which the source files are stored.
     */
    bucketPrefix?: string;
    /**
     * If the flow should fail after the first instance of a failure when attempting to place data in the destination.
     */
    failOnFirstDestinationError?: boolean;
}

export interface FlowSourceFlowConfig {
    /**
     * API version that the destination connector uses.
     */
    apiVersion?: string;
    /**
     * Name of the connector profile. This name must be unique for each connector profile in the AWS account.
     */
    connectorProfileName?: string;
    /**
     * Type of connector, such as Salesforce, Amplitude, and so on. Valid values are `Salesforce`, `Singular`, `Slack`, `Redshift`, `S3`, `Marketo`, `Googleanalytics`, `Zendesk`, `Servicenow`, `Datadog`, `Trendmicro`, `Snowflake`, `Dynatrace`, `Infornexus`, `Amplitude`, `Veeva`, `EventBridge`, `LookoutMetrics`, `Upsolver`, `Honeycode`, `CustomerProfiles`, `SAPOData`, and `CustomConnector`.
     */
    connectorType: string;
    /**
     * Defines the configuration for a scheduled incremental data pull. If a valid configuration is provided, the fields specified in the configuration are used when querying for the incremental data pull. See Incremental Pull Config for more details.
     */
    incrementalPullConfig?: outputs.appflow.FlowSourceFlowConfigIncrementalPullConfig;
    /**
     * Information that is required to query a particular source connector. See Source Connector Properties for details.
     */
    sourceConnectorProperties: outputs.appflow.FlowSourceFlowConfigSourceConnectorProperties;
}

export interface FlowSourceFlowConfigIncrementalPullConfig {
    /**
     * Field that specifies the date time or timestamp field as the criteria to use when importing incremental records from the source.
     */
    datetimeTypeFieldName?: string;
}

export interface FlowSourceFlowConfigSourceConnectorProperties {
    /**
     * Operation to be performed on the provided Amplitude source fields. The only valid value is `BETWEEN`.
     */
    amplitude?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesAmplitude;
    /**
     * Operators supported by the custom connector. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    customConnector?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesCustomConnector;
    /**
     * Operation to be performed on the provided Datadog source fields. Valid values are `PROJECTION`, `BETWEEN`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    datadog?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesDatadog;
    /**
     * Operation to be performed on the provided Dynatrace source fields. Valid values are `PROJECTION`, `BETWEEN`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    dynatrace?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesDynatrace;
    /**
     * Operation to be performed on the provided Google Analytics source fields. Valid values are `PROJECTION` and `BETWEEN`.
     */
    googleAnalytics?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesGoogleAnalytics;
    /**
     * Operation to be performed on the provided Infor Nexus source fields. Valid values are `PROJECTION`, `BETWEEN`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    inforNexus?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesInforNexus;
    /**
     * Operation to be performed on the provided Marketo source fields. Valid values are `PROJECTION`, `BETWEEN`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    marketo?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesMarketo;
    /**
     * Operation to be performed on the provided Amazon S3 source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    s3?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesS3;
    /**
     * Operation to be performed on the provided Salesforce source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    salesforce?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesSalesforce;
    /**
     * Operation to be performed on the provided SAPOData source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    sapoData?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesSapoData;
    /**
     * Operation to be performed on the provided ServiceNow source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    serviceNow?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesServiceNow;
    /**
     * Operation to be performed on the provided Singular source fields. Valid values are `PROJECTION`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    singular?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesSingular;
    /**
     * Operation to be performed on the provided Slack source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    slack?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesSlack;
    /**
     * Operation to be performed on the provided Trend Micro source fields. Valid values are `PROJECTION`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    trendmicro?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesTrendmicro;
    /**
     * Operation to be performed on the provided Veeva source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    veeva?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesVeeva;
    /**
     * Operation to be performed on the provided Zendesk source fields. Valid values are `PROJECTION`, `GREATER_THAN`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    zendesk?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesZendesk;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesAmplitude {
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesCustomConnector {
    /**
     * Custom properties that are specific to the connector when it's used as a source in the flow. Maximum of 50 items.
     */
    customProperties?: {[key: string]: string};
    /**
     * Entity specified in the custom connector as a source in the flow.
     */
    entityName: string;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesDatadog {
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesDynatrace {
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesGoogleAnalytics {
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesInforNexus {
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesMarketo {
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesS3 {
    /**
     * Amazon S3 bucket name where the source files are stored.
     */
    bucketName: string;
    /**
     * Object key for the Amazon S3 bucket in which the source files are stored.
     */
    bucketPrefix?: string;
    /**
     * When you use Amazon S3 as the source, the configuration format that you provide the flow input data. See S3 Input Format Config for details.
     */
    s3InputFormatConfig?: outputs.appflow.FlowSourceFlowConfigSourceConnectorPropertiesS3S3InputFormatConfig;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesS3S3InputFormatConfig {
    /**
     * File type that Amazon AppFlow gets from your Amazon S3 bucket. Valid values are `CSV` and `JSON`.
     */
    s3InputFileType?: string;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesSalesforce {
    /**
     * Flag that enables dynamic fetching of new (recently added) fields in the Salesforce objects while running a flow.
     */
    enableDynamicFieldUpdate?: boolean;
    /**
     * Whether Amazon AppFlow includes deleted files in the flow run.
     */
    includeDeletedRecords?: boolean;
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesSapoData {
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesServiceNow {
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesSingular {
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesSlack {
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesTrendmicro {
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesVeeva {
    /**
     * Document type specified in the Veeva document extract flow.
     */
    documentType?: string;
    /**
     * Boolean value to include All Versions of files in Veeva document extract flow.
     */
    includeAllVersions?: boolean;
    /**
     * Boolean value to include file renditions in Veeva document extract flow.
     */
    includeRenditions?: boolean;
    /**
     * Boolean value to include source files in Veeva document extract flow.
     */
    includeSourceFiles?: boolean;
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowSourceFlowConfigSourceConnectorPropertiesZendesk {
    /**
     * Object specified in the Veeva flow source.
     */
    object: string;
}

export interface FlowTask {
    /**
     * Operation to be performed on the provided source fields. See Connector Operator for details.
     */
    connectorOperators?: outputs.appflow.FlowTaskConnectorOperator[];
    /**
     * Field in a destination connector, or a field value against which Amazon AppFlow validates a source field.
     */
    destinationField?: string;
    /**
     * Source fields to which a particular task is applied.
     */
    sourceFields: string[];
    /**
     * Map used to store task-related information. The execution service looks for particular information based on the `TaskType`. Valid keys are `VALUE`, `VALUES`, `DATA_TYPE`, `UPPER_BOUND`, `LOWER_BOUND`, `SOURCE_DATA_TYPE`, `DESTINATION_DATA_TYPE`, `VALIDATION_ACTION`, `MASK_VALUE`, `MASK_LENGTH`, `TRUNCATE_LENGTH`, `MATH_OPERATION_FIELDS_ORDER`, `CONCAT_FORMAT`, `SUBFIELD_CATEGORY_MAP`, and `EXCLUDE_SOURCE_FIELDS_LIST`.
     */
    taskProperties?: {[key: string]: string};
    /**
     * Particular task implementation that Amazon AppFlow performs. Valid values are `Arithmetic`, `Filter`, `Map`, `Map_all`, `Mask`, `Merge`, `Passthrough`, `Truncate`, and `Validate`.
     */
    taskType: string;
}

export interface FlowTaskConnectorOperator {
    /**
     * Operation to be performed on the provided Amplitude source fields. The only valid value is `BETWEEN`.
     */
    amplitude?: string;
    /**
     * Operators supported by the custom connector. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    customConnector?: string;
    /**
     * Operation to be performed on the provided Datadog source fields. Valid values are `PROJECTION`, `BETWEEN`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    datadog?: string;
    /**
     * Operation to be performed on the provided Dynatrace source fields. Valid values are `PROJECTION`, `BETWEEN`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    dynatrace?: string;
    /**
     * Operation to be performed on the provided Google Analytics source fields. Valid values are `PROJECTION` and `BETWEEN`.
     */
    googleAnalytics?: string;
    /**
     * Operation to be performed on the provided Infor Nexus source fields. Valid values are `PROJECTION`, `BETWEEN`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    inforNexus?: string;
    /**
     * Operation to be performed on the provided Marketo source fields. Valid values are `PROJECTION`, `BETWEEN`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    marketo?: string;
    /**
     * Operation to be performed on the provided Amazon S3 source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    s3?: string;
    /**
     * Operation to be performed on the provided Salesforce source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    salesforce?: string;
    /**
     * Operation to be performed on the provided SAPOData source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    sapoData?: string;
    /**
     * Operation to be performed on the provided ServiceNow source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    serviceNow?: string;
    /**
     * Operation to be performed on the provided Singular source fields. Valid values are `PROJECTION`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    singular?: string;
    /**
     * Operation to be performed on the provided Slack source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    slack?: string;
    /**
     * Operation to be performed on the provided Trend Micro source fields. Valid values are `PROJECTION`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    trendmicro?: string;
    /**
     * Operation to be performed on the provided Veeva source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    veeva?: string;
    /**
     * Operation to be performed on the provided Zendesk source fields. Valid values are `PROJECTION`, `GREATER_THAN`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
     */
    zendesk?: string;
}

export interface FlowTriggerConfig {
    /**
     * Configuration details of a schedule-triggered flow as defined by the user. Currently, these settings only apply to the `Scheduled` trigger type. See Scheduled Trigger Properties for details.
     */
    triggerProperties: outputs.appflow.FlowTriggerConfigTriggerProperties;
    /**
     * Type of flow trigger. Valid values are `Scheduled`, `Event`, and `OnDemand`.
     */
    triggerType: string;
}

export interface FlowTriggerConfigTriggerProperties {
    scheduled?: outputs.appflow.FlowTriggerConfigTriggerPropertiesScheduled;
}

export interface FlowTriggerConfigTriggerPropertiesScheduled {
    /**
     * Whether a scheduled flow has an incremental data transfer or a complete data transfer for each flow run. Valid values are `Incremental` and `Complete`.
     */
    dataPullMode?: string;
    /**
     * Date range for the records to import from the connector in the first flow run. Must be a valid RFC3339 timestamp.
     */
    firstExecutionFrom?: string;
    /**
     * Scheduled end time for a schedule-triggered flow. Must be a valid RFC3339 timestamp.
     */
    scheduleEndTime?: string;
    /**
     * Scheduling expression that determines the rate at which the schedule will run, for example `rate(5minutes)`.
     */
    scheduleExpression: string;
    /**
     * Optional offset that is added to the time interval for a schedule-triggered flow. Maximum value of 36000.
     */
    scheduleOffset?: number;
    /**
     * Scheduled start time for a schedule-triggered flow. Must be a valid RFC3339 timestamp.
     */
    scheduleStartTime?: string;
    /**
     * Time zone used when referring to the date and time of a scheduled-triggered flow, such as `America/New_York`.
     */
    timezone?: string;
}

