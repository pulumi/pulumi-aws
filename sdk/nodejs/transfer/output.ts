// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "./input";
import * as outputs from "./output";

export interface AccessHomeDirectoryMapping {
    /**
     * Represents an entry and a target.
     */
    entry: string;
    /**
     * Represents the map target.
     */
    target: string;
}
export interface AccessPosixProfile {
    /**
     * The POSIX group ID used for all EFS operations by this user.
     */
    gid: number;
    /**
     * The secondary POSIX group IDs used for all EFS operations by this user.
     */
    secondaryGids?: number[];
    /**
     * The POSIX user ID used for all EFS operations by this user.
     */
    uid: number;
}
export interface ConnectorAs2Config {
    compression: string;
    encryptionAlgorithm: string;
    localProfileId: string;
    mdnResponse: string;
    mdnSigningAlgorithm?: string;
    messageSubject?: string;
    partnerProfileId: string;
    signingAlgorithm: string;
}
export interface ConnectorSftpConfig {
    trustedHostKeys?: string[];
    userSecretId?: string;
}
export interface ServerEndpointDetails {
    /**
     * A list of address allocation IDs that are required to attach an Elastic IP address to your SFTP server's endpoint. This property can only be used when `endpointType` is set to `VPC`.
     */
    addressAllocationIds?: string[];
    /**
     * A list of security groups IDs that are available to attach to your server's endpoint. If no security groups are specified, the VPC's default security groups are automatically assigned to your endpoint. This property can only be used when `endpointType` is set to `VPC`.
     */
    securityGroupIds: string[];
    /**
     * A list of subnet IDs that are required to host your SFTP server endpoint in your VPC. This property can only be used when `endpointType` is set to `VPC`.
     */
    subnetIds?: string[];
    /**
     * The ID of the VPC endpoint. This property can only be used when `endpointType` is set to `VPC_ENDPOINT`
     */
    vpcEndpointId: string;
    /**
     * The VPC ID of the virtual private cloud in which the SFTP server's endpoint will be hosted. This property can only be used when `endpointType` is set to `VPC`.
     */
    vpcId?: string;
}
export interface ServerProtocolDetails {
    /**
     * Indicates the transport method for the AS2 messages. Currently, only `HTTP` is supported.
     */
    as2Transports: string[];
    /**
     * Indicates passive mode, for FTP and FTPS protocols. Enter a single IPv4 address, such as the public IP address of a firewall, router, or load balancer.
     */
    passiveIp: string;
    /**
     * Use to ignore the error that is generated when the client attempts to use `SETSTAT` on a file you are uploading to an S3 bucket. Valid values: `DEFAULT`, `ENABLE_NO_OP`.
     */
    setStatOption: string;
    /**
     * A property used with Transfer Family servers that use the FTPS protocol. Provides a mechanism to resume or share a negotiated secret key between the control and data connection for an FTPS session. Valid values: `DISABLED`, `ENABLED`, `ENFORCED`.
     */
    tlsSessionResumptionMode: string;
}
export interface ServerS3StorageOptions {
    /**
     * Specifies whether or not performance for your Amazon S3 directories is optimized. Valid values are `DISABLED`, `ENABLED`.
     *
     * By default, home directory mappings have a `TYPE` of `DIRECTORY`. If you enable this option, you would then need to explicitly set the `HomeDirectoryMapEntry` Type to `FILE` if you want a mapping to have a file target. See [Using logical directories to simplify your Transfer Family directory structures](https://docs.aws.amazon.com/transfer/latest/userguide/logical-dir-mappings.html) for details.
     */
    directoryListingOptimization: string;
}
export interface ServerWorkflowDetails {
    /**
     * A trigger that starts a workflow if a file is only partially uploaded. See Workflow Detail below. See `onPartialUpload` block below for details.
     */
    onPartialUpload?: outputs.ServerWorkflowDetailsOnPartialUpload;
    /**
     * A trigger that starts a workflow: the workflow begins to execute after a file is uploaded. See `onUpload` block below for details.
     */
    onUpload?: outputs.ServerWorkflowDetailsOnUpload;
}
export interface ServerWorkflowDetailsOnPartialUpload {
    /**
     * Includes the necessary permissions for S3, EFS, and Lambda operations that Transfer can assume, so that all workflow steps can operate on the required resources.
     */
    executionRole: string;
    /**
     * A unique identifier for the workflow.
     */
    workflowId: string;
}
export interface ServerWorkflowDetailsOnUpload {
    /**
     * Includes the necessary permissions for S3, EFS, and Lambda operations that Transfer can assume, so that all workflow steps can operate on the required resources.
     */
    executionRole: string;
    /**
     * A unique identifier for the workflow.
     */
    workflowId: string;
}
export interface UserHomeDirectoryMapping {
    /**
     * Represents an entry and a target.
     */
    entry: string;
    /**
     * Represents the map target.
     *
     * The `Restricted` option is achieved using the following mapping:
     *
     * ```
     * home_directory_mappings {
     * entry  = "/"
     * target = "/${aws_s3_bucket.foo.id}/$${Transfer:UserName}"
     * }
     * ```
     */
    target: string;
}
export interface UserPosixProfile {
    /**
     * The POSIX group ID used for all EFS operations by this user.
     */
    gid: number;
    /**
     * The secondary POSIX group IDs used for all EFS operations by this user.
     */
    secondaryGids?: number[];
    /**
     * The POSIX user ID used for all EFS operations by this user.
     */
    uid: number;
}
export interface WorkflowOnExceptionStep {
    copyStepDetails?: outputs.WorkflowOnExceptionStepCopyStepDetails;
    customStepDetails?: outputs.WorkflowOnExceptionStepCustomStepDetails;
    decryptStepDetails?: outputs.WorkflowOnExceptionStepDecryptStepDetails;
    deleteStepDetails?: outputs.WorkflowOnExceptionStepDeleteStepDetails;
    tagStepDetails?: outputs.WorkflowOnExceptionStepTagStepDetails;
    type: string;
}
export interface WorkflowOnExceptionStepCopyStepDetails {
    /**
     * Specifies the location for the file being copied. Use ${Transfer:username} in this field to parametrize the destination prefix by username.
     */
    destinationFileLocation?: outputs.WorkflowOnExceptionStepCopyStepDetailsDestinationFileLocation;
    /**
     * The name of the step, used as an identifier.
     */
    name?: string;
    /**
     * A flag that indicates whether or not to overwrite an existing file of the same name. The default is `FALSE`. Valid values are `TRUE` and `FALSE`.
     */
    overwriteExisting?: string;
    /**
     * Specifies which file to use as input to the workflow step: either the output from the previous step, or the originally uploaded file for the workflow. Enter ${previous.file} to use the previous file as the input. In this case, this workflow step uses the output file from the previous workflow step as input. This is the default value. Enter ${original.file} to use the originally-uploaded file location as input for this step.
     */
    sourceFileLocation?: string;
}
export interface WorkflowOnExceptionStepCopyStepDetailsDestinationFileLocation {
    /**
     * Specifies the details for the EFS file being copied.
     */
    efsFileLocation?: outputs.WorkflowOnExceptionStepCopyStepDetailsDestinationFileLocationEfsFileLocation;
    /**
     * Specifies the details for the S3 file being copied.
     */
    s3FileLocation?: outputs.WorkflowOnExceptionStepCopyStepDetailsDestinationFileLocationS3FileLocation;
}
export interface WorkflowOnExceptionStepCopyStepDetailsDestinationFileLocationEfsFileLocation {
    /**
     * The ID of the file system, assigned by Amazon EFS.
     */
    fileSystemId?: string;
    /**
     * The pathname for the folder being used by a workflow.
     */
    path?: string;
}
export interface WorkflowOnExceptionStepCopyStepDetailsDestinationFileLocationS3FileLocation {
    /**
     * Specifies the S3 bucket for the customer input file.
     */
    bucket?: string;
    /**
     * The name assigned to the file when it was created in S3. You use the object key to retrieve the object.
     */
    key?: string;
}
export interface WorkflowOnExceptionStepCustomStepDetails {
    /**
     * The name of the step, used as an identifier.
     */
    name?: string;
    /**
     * Specifies which file to use as input to the workflow step: either the output from the previous step, or the originally uploaded file for the workflow. Enter ${previous.file} to use the previous file as the input. In this case, this workflow step uses the output file from the previous workflow step as input. This is the default value. Enter ${original.file} to use the originally-uploaded file location as input for this step.
     */
    sourceFileLocation?: string;
    /**
     * The ARN for the lambda function that is being called.
     */
    target?: string;
    /**
     * Timeout, in seconds, for the step.
     */
    timeoutSeconds?: number;
}
export interface WorkflowOnExceptionStepDecryptStepDetails {
    /**
     * Specifies the location for the file being copied. Use ${Transfer:username} in this field to parametrize the destination prefix by username.
     */
    destinationFileLocation?: outputs.WorkflowOnExceptionStepDecryptStepDetailsDestinationFileLocation;
    /**
     * The name of the step, used as an identifier.
     */
    name?: string;
    /**
     * A flag that indicates whether or not to overwrite an existing file of the same name. The default is `FALSE`. Valid values are `TRUE` and `FALSE`.
     */
    overwriteExisting?: string;
    /**
     * Specifies which file to use as input to the workflow step: either the output from the previous step, or the originally uploaded file for the workflow. Enter ${previous.file} to use the previous file as the input. In this case, this workflow step uses the output file from the previous workflow step as input. This is the default value. Enter ${original.file} to use the originally-uploaded file location as input for this step.
     */
    sourceFileLocation?: string;
    /**
     * The type of encryption used. Currently, this value must be `"PGP"`.
     */
    type: string;
}
export interface WorkflowOnExceptionStepDecryptStepDetailsDestinationFileLocation {
    /**
     * Specifies the details for the EFS file being copied.
     */
    efsFileLocation?: outputs.WorkflowOnExceptionStepDecryptStepDetailsDestinationFileLocationEfsFileLocation;
    /**
     * Specifies the details for the S3 file being copied.
     */
    s3FileLocation?: outputs.WorkflowOnExceptionStepDecryptStepDetailsDestinationFileLocationS3FileLocation;
}
export interface WorkflowOnExceptionStepDecryptStepDetailsDestinationFileLocationEfsFileLocation {
    /**
     * The ID of the file system, assigned by Amazon EFS.
     */
    fileSystemId?: string;
    /**
     * The pathname for the folder being used by a workflow.
     */
    path?: string;
}
export interface WorkflowOnExceptionStepDecryptStepDetailsDestinationFileLocationS3FileLocation {
    /**
     * Specifies the S3 bucket for the customer input file.
     */
    bucket?: string;
    /**
     * The name assigned to the file when it was created in S3. You use the object key to retrieve the object.
     */
    key?: string;
}
export interface WorkflowOnExceptionStepDeleteStepDetails {
    /**
     * The name of the step, used as an identifier.
     */
    name?: string;
    /**
     * Specifies which file to use as input to the workflow step: either the output from the previous step, or the originally uploaded file for the workflow. Enter ${previous.file} to use the previous file as the input. In this case, this workflow step uses the output file from the previous workflow step as input. This is the default value. Enter ${original.file} to use the originally-uploaded file location as input for this step.
     */
    sourceFileLocation?: string;
}
export interface WorkflowOnExceptionStepTagStepDetails {
    /**
     * The name of the step, used as an identifier.
     */
    name?: string;
    /**
     * Specifies which file to use as input to the workflow step: either the output from the previous step, or the originally uploaded file for the workflow. Enter ${previous.file} to use the previous file as the input. In this case, this workflow step uses the output file from the previous workflow step as input. This is the default value. Enter ${original.file} to use the originally-uploaded file location as input for this step.
     */
    sourceFileLocation?: string;
    /**
     * Array that contains from 1 to 10 key/value pairs. See S3 Tags below.
     */
    tags?: outputs.WorkflowOnExceptionStepTagStepDetailsTag[];
}
export interface WorkflowOnExceptionStepTagStepDetailsTag {
    key: string;
    value: string;
}
export interface WorkflowStep {
    copyStepDetails?: outputs.WorkflowStepCopyStepDetails;
    customStepDetails?: outputs.WorkflowStepCustomStepDetails;
    decryptStepDetails?: outputs.WorkflowStepDecryptStepDetails;
    deleteStepDetails?: outputs.WorkflowStepDeleteStepDetails;
    tagStepDetails?: outputs.WorkflowStepTagStepDetails;
    type: string;
}
export interface WorkflowStepCopyStepDetails {
    /**
     * Specifies the location for the file being copied. Use ${Transfer:username} in this field to parametrize the destination prefix by username.
     */
    destinationFileLocation?: outputs.WorkflowStepCopyStepDetailsDestinationFileLocation;
    /**
     * The name of the step, used as an identifier.
     */
    name?: string;
    /**
     * A flag that indicates whether or not to overwrite an existing file of the same name. The default is `FALSE`. Valid values are `TRUE` and `FALSE`.
     */
    overwriteExisting?: string;
    /**
     * Specifies which file to use as input to the workflow step: either the output from the previous step, or the originally uploaded file for the workflow. Enter ${previous.file} to use the previous file as the input. In this case, this workflow step uses the output file from the previous workflow step as input. This is the default value. Enter ${original.file} to use the originally-uploaded file location as input for this step.
     */
    sourceFileLocation?: string;
}
export interface WorkflowStepCopyStepDetailsDestinationFileLocation {
    /**
     * Specifies the details for the EFS file being copied.
     */
    efsFileLocation?: outputs.WorkflowStepCopyStepDetailsDestinationFileLocationEfsFileLocation;
    /**
     * Specifies the details for the S3 file being copied.
     */
    s3FileLocation?: outputs.WorkflowStepCopyStepDetailsDestinationFileLocationS3FileLocation;
}
export interface WorkflowStepCopyStepDetailsDestinationFileLocationEfsFileLocation {
    /**
     * The ID of the file system, assigned by Amazon EFS.
     */
    fileSystemId?: string;
    /**
     * The pathname for the folder being used by a workflow.
     */
    path?: string;
}
export interface WorkflowStepCopyStepDetailsDestinationFileLocationS3FileLocation {
    /**
     * Specifies the S3 bucket for the customer input file.
     */
    bucket?: string;
    /**
     * The name assigned to the file when it was created in S3. You use the object key to retrieve the object.
     */
    key?: string;
}
export interface WorkflowStepCustomStepDetails {
    /**
     * The name of the step, used as an identifier.
     */
    name?: string;
    /**
     * Specifies which file to use as input to the workflow step: either the output from the previous step, or the originally uploaded file for the workflow. Enter ${previous.file} to use the previous file as the input. In this case, this workflow step uses the output file from the previous workflow step as input. This is the default value. Enter ${original.file} to use the originally-uploaded file location as input for this step.
     */
    sourceFileLocation?: string;
    /**
     * The ARN for the lambda function that is being called.
     */
    target?: string;
    /**
     * Timeout, in seconds, for the step.
     */
    timeoutSeconds?: number;
}
export interface WorkflowStepDecryptStepDetails {
    /**
     * Specifies the location for the file being copied. Use ${Transfer:username} in this field to parametrize the destination prefix by username.
     */
    destinationFileLocation?: outputs.WorkflowStepDecryptStepDetailsDestinationFileLocation;
    /**
     * The name of the step, used as an identifier.
     */
    name?: string;
    /**
     * A flag that indicates whether or not to overwrite an existing file of the same name. The default is `FALSE`. Valid values are `TRUE` and `FALSE`.
     */
    overwriteExisting?: string;
    /**
     * Specifies which file to use as input to the workflow step: either the output from the previous step, or the originally uploaded file for the workflow. Enter ${previous.file} to use the previous file as the input. In this case, this workflow step uses the output file from the previous workflow step as input. This is the default value. Enter ${original.file} to use the originally-uploaded file location as input for this step.
     */
    sourceFileLocation?: string;
    /**
     * The type of encryption used. Currently, this value must be `"PGP"`.
     */
    type: string;
}
export interface WorkflowStepDecryptStepDetailsDestinationFileLocation {
    /**
     * Specifies the details for the EFS file being copied.
     */
    efsFileLocation?: outputs.WorkflowStepDecryptStepDetailsDestinationFileLocationEfsFileLocation;
    /**
     * Specifies the details for the S3 file being copied.
     */
    s3FileLocation?: outputs.WorkflowStepDecryptStepDetailsDestinationFileLocationS3FileLocation;
}
export interface WorkflowStepDecryptStepDetailsDestinationFileLocationEfsFileLocation {
    /**
     * The ID of the file system, assigned by Amazon EFS.
     */
    fileSystemId?: string;
    /**
     * The pathname for the folder being used by a workflow.
     */
    path?: string;
}
export interface WorkflowStepDecryptStepDetailsDestinationFileLocationS3FileLocation {
    /**
     * Specifies the S3 bucket for the customer input file.
     */
    bucket?: string;
    /**
     * The name assigned to the file when it was created in S3. You use the object key to retrieve the object.
     */
    key?: string;
}
export interface WorkflowStepDeleteStepDetails {
    /**
     * The name of the step, used as an identifier.
     */
    name?: string;
    /**
     * Specifies which file to use as input to the workflow step: either the output from the previous step, or the originally uploaded file for the workflow. Enter ${previous.file} to use the previous file as the input. In this case, this workflow step uses the output file from the previous workflow step as input. This is the default value. Enter ${original.file} to use the originally-uploaded file location as input for this step.
     */
    sourceFileLocation?: string;
}
export interface WorkflowStepTagStepDetails {
    /**
     * The name of the step, used as an identifier.
     */
    name?: string;
    /**
     * Specifies which file to use as input to the workflow step: either the output from the previous step, or the originally uploaded file for the workflow. Enter ${previous.file} to use the previous file as the input. In this case, this workflow step uses the output file from the previous workflow step as input. This is the default value. Enter ${original.file} to use the originally-uploaded file location as input for this step.
     */
    sourceFileLocation?: string;
    /**
     * Array that contains from 1 to 10 key/value pairs. See S3 Tags below.
     */
    tags?: outputs.WorkflowStepTagStepDetailsTag[];
}
export interface WorkflowStepTagStepDetailsTag {
    key: string;
    value: string;
}
