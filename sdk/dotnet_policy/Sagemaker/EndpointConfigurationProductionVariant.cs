// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;
using Pulumi;

namespace Pulumi.PolicyPacks.Aws.Sagemaker
{
    [PolicyResourceType("aws:sagemaker/EndpointConfigurationProductionVariant:EndpointConfigurationProductionVariant")]
    public sealed class EndpointConfigurationProductionVariant
    {
        /// <summary>
        /// The size of the Elastic Inference (EI) instance to use for the production variant.
        /// </summary>
        [Input("acceleratorType")]
        public string? AcceleratorType;

        /// <summary>
        /// The timeout value, in seconds, for your inference container to pass health check by SageMaker AI Hosting. For more information about health check, see [How Your Container Should Respond to Health Check (Ping) Requests](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-algo-ping-requests). Valid values between `60` and `3600`.
        /// </summary>
        [Input("containerStartupHealthCheckTimeoutInSeconds")]
        public int? ContainerStartupHealthCheckTimeoutInSeconds;

        /// <summary>
        /// Specifies configuration for a core dump from the model container when the process crashes. Fields are documented below.
        /// </summary>
        [Input("coreDumpConfig")]
        public EndpointConfigurationProductionVariantCoreDumpConfig? CoreDumpConfig;

        /// <summary>
        /// You can use this parameter to turn on native Amazon Web Services Systems Manager (SSM) access for a production variant behind an endpoint. By default, SSM access is disabled for all production variants behind an endpoints.
        /// </summary>
        [Input("enableSsmAccess")]
        public bool? EnableSsmAccess;

        /// <summary>
        /// Specifies an option from a collection of preconfigured Amazon Machine Image (AMI) images. Each image is configured by Amazon Web Services with a set of software and driver versions. Amazon Web Services optimizes these configurations for different machine learning workloads.
        /// </summary>
        [Input("inferenceAmiVersion")]
        public string? InferenceAmiVersion;

        /// <summary>
        /// Initial number of instances used for auto-scaling.
        /// </summary>
        [Input("initialInstanceCount")]
        public int? InitialInstanceCount;

        /// <summary>
        /// Determines initial traffic distribution among all of the models that you specify in the endpoint configuration. If unspecified, it defaults to `1.0`.
        /// </summary>
        [Input("initialVariantWeight")]
        public double? InitialVariantWeight;

        /// <summary>
        /// The type of instance to start.
        /// </summary>
        [Input("instanceType")]
        public string? InstanceType;

        /// <summary>
        /// Settings that control the range in the number of instances that the endpoint provisions as it scales up or down to accommodate traffic.
        /// </summary>
        [Input("managedInstanceScaling")]
        public EndpointConfigurationProductionVariantManagedInstanceScaling? ManagedInstanceScaling;

        /// <summary>
        /// The timeout value, in seconds, to download and extract the model that you want to host from Amazon S3 to the individual inference instance associated with this production variant. Valid values between `60` and `3600`.
        /// </summary>
        [Input("modelDataDownloadTimeoutInSeconds")]
        public int? ModelDataDownloadTimeoutInSeconds;

        /// <summary>
        /// The name of the model to use.
        /// </summary>
        [Input("modelName")]
        public string? ModelName;

        /// <summary>
        /// Sets how the endpoint routes incoming traffic. See routing_config below.
        /// </summary>
        [Input("routingConfigs")]
        public List<EndpointConfigurationProductionVariantRoutingConfig>? RoutingConfigs;

        /// <summary>
        /// Specifies configuration for how an endpoint performs asynchronous inference.
        /// </summary>
        [Input("serverlessConfig")]
        public EndpointConfigurationProductionVariantServerlessConfig? ServerlessConfig;

        /// <summary>
        /// The name of the variant. If omitted, this provider will assign a random, unique name.
        /// </summary>
        [Input("variantName")]
        public string? VariantName;

        /// <summary>
        /// The size, in GB, of the ML storage volume attached to individual inference instance associated with the production variant. Valid values between `1` and `512`.
        /// </summary>
        [Input("volumeSizeInGb")]
        public int? VolumeSizeInGb;
    }
}
