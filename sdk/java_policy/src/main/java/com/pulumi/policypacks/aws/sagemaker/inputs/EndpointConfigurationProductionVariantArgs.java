// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.policypacks.aws.sagemaker.inputs;

import com.pulumi.core.UndeferrableValue;
import com.pulumi.policypacks.aws.sagemaker.inputs.EndpointConfigurationProductionVariantCoreDumpConfigArgs;
import com.pulumi.policypacks.aws.sagemaker.inputs.EndpointConfigurationProductionVariantManagedInstanceScalingArgs;
import com.pulumi.policypacks.aws.sagemaker.inputs.EndpointConfigurationProductionVariantRoutingConfigArgs;
import com.pulumi.policypacks.aws.sagemaker.inputs.EndpointConfigurationProductionVariantServerlessConfigArgs;
import java.lang.Boolean;
import java.lang.Double;
import java.lang.Integer;
import java.lang.String;
import java.util.List;
import javax.annotation.Nullable;


public final class EndpointConfigurationProductionVariantArgs {

    /**
     * The size of the Elastic Inference (EI) instance to use for the production variant.
     * 
     */
    private UndeferrableValue<String> acceleratorType;

    public String acceleratorType() {
        if (acceleratorType == null) return null;
        return acceleratorType.getValue("EndpointConfigurationProductionVariantArgs.acceleratorType");
    }

    /**
     * The timeout value, in seconds, for your inference container to pass health check by SageMaker AI Hosting. For more information about health check, see [How Your Container Should Respond to Health Check (Ping) Requests](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-algo-ping-requests). Valid values between `60` and `3600`.
     * 
     */
    private UndeferrableValue<Integer> containerStartupHealthCheckTimeoutInSeconds;

    public Integer containerStartupHealthCheckTimeoutInSeconds() {
        if (containerStartupHealthCheckTimeoutInSeconds == null) return null;
        return containerStartupHealthCheckTimeoutInSeconds.getValue("EndpointConfigurationProductionVariantArgs.containerStartupHealthCheckTimeoutInSeconds");
    }

    /**
     * Specifies configuration for a core dump from the model container when the process crashes. Fields are documented below.
     * 
     */
    private UndeferrableValue<EndpointConfigurationProductionVariantCoreDumpConfigArgs> coreDumpConfig;

    public EndpointConfigurationProductionVariantCoreDumpConfigArgs coreDumpConfig() {
        if (coreDumpConfig == null) return null;
        return coreDumpConfig.getValue("EndpointConfigurationProductionVariantArgs.coreDumpConfig");
    }

    /**
     * You can use this parameter to turn on native Amazon Web Services Systems Manager (SSM) access for a production variant behind an endpoint. By default, SSM access is disabled for all production variants behind an endpoints.
     * 
     */
    private UndeferrableValue<Boolean> enableSsmAccess;

    public Boolean enableSsmAccess() {
        if (enableSsmAccess == null) return null;
        return enableSsmAccess.getValue("EndpointConfigurationProductionVariantArgs.enableSsmAccess");
    }

    /**
     * Specifies an option from a collection of preconfigured Amazon Machine Image (AMI) images. Each image is configured by Amazon Web Services with a set of software and driver versions. Amazon Web Services optimizes these configurations for different machine learning workloads.
     * 
     */
    private UndeferrableValue<String> inferenceAmiVersion;

    public String inferenceAmiVersion() {
        if (inferenceAmiVersion == null) return null;
        return inferenceAmiVersion.getValue("EndpointConfigurationProductionVariantArgs.inferenceAmiVersion");
    }

    /**
     * Initial number of instances used for auto-scaling.
     * 
     */
    private UndeferrableValue<Integer> initialInstanceCount;

    public Integer initialInstanceCount() {
        if (initialInstanceCount == null) return null;
        return initialInstanceCount.getValue("EndpointConfigurationProductionVariantArgs.initialInstanceCount");
    }

    /**
     * Determines initial traffic distribution among all of the models that you specify in the endpoint configuration. If unspecified, it defaults to `1.0`.
     * 
     */
    private UndeferrableValue<Double> initialVariantWeight;

    public Double initialVariantWeight() {
        if (initialVariantWeight == null) return null;
        return initialVariantWeight.getValue("EndpointConfigurationProductionVariantArgs.initialVariantWeight");
    }

    /**
     * The type of instance to start.
     * 
     */
    private UndeferrableValue<String> instanceType;

    public String instanceType() {
        if (instanceType == null) return null;
        return instanceType.getValue("EndpointConfigurationProductionVariantArgs.instanceType");
    }

    /**
     * Settings that control the range in the number of instances that the endpoint provisions as it scales up or down to accommodate traffic.
     * 
     */
    private UndeferrableValue<EndpointConfigurationProductionVariantManagedInstanceScalingArgs> managedInstanceScaling;

    public EndpointConfigurationProductionVariantManagedInstanceScalingArgs managedInstanceScaling() {
        if (managedInstanceScaling == null) return null;
        return managedInstanceScaling.getValue("EndpointConfigurationProductionVariantArgs.managedInstanceScaling");
    }

    /**
     * The timeout value, in seconds, to download and extract the model that you want to host from Amazon S3 to the individual inference instance associated with this production variant. Valid values between `60` and `3600`.
     * 
     */
    private UndeferrableValue<Integer> modelDataDownloadTimeoutInSeconds;

    public Integer modelDataDownloadTimeoutInSeconds() {
        if (modelDataDownloadTimeoutInSeconds == null) return null;
        return modelDataDownloadTimeoutInSeconds.getValue("EndpointConfigurationProductionVariantArgs.modelDataDownloadTimeoutInSeconds");
    }

    /**
     * The name of the model to use.
     * 
     */
    private UndeferrableValue<String> modelName;

    public String modelName() {
        if (modelName == null) return null;
        return modelName.getValue("EndpointConfigurationProductionVariantArgs.modelName");
    }

    /**
     * Sets how the endpoint routes incoming traffic. See routing_config below.
     * 
     */
    private UndeferrableValue<List<EndpointConfigurationProductionVariantRoutingConfigArgs>> routingConfigs;

    public List<EndpointConfigurationProductionVariantRoutingConfigArgs> routingConfigs() {
        if (routingConfigs == null) return null;
        return routingConfigs.getValue("EndpointConfigurationProductionVariantArgs.routingConfigs");
    }

    /**
     * Specifies configuration for how an endpoint performs asynchronous inference.
     * 
     */
    private UndeferrableValue<EndpointConfigurationProductionVariantServerlessConfigArgs> serverlessConfig;

    public EndpointConfigurationProductionVariantServerlessConfigArgs serverlessConfig() {
        if (serverlessConfig == null) return null;
        return serverlessConfig.getValue("EndpointConfigurationProductionVariantArgs.serverlessConfig");
    }

    /**
     * The name of the variant. If omitted, this provider will assign a random, unique name.
     * 
     */
    private UndeferrableValue<String> variantName;

    public String variantName() {
        if (variantName == null) return null;
        return variantName.getValue("EndpointConfigurationProductionVariantArgs.variantName");
    }

    /**
     * The size, in GB, of the ML storage volume attached to individual inference instance associated with the production variant. Valid values between `1` and `512`.
     * 
     */
    private UndeferrableValue<Integer> volumeSizeInGb;

    public Integer volumeSizeInGb() {
        if (volumeSizeInGb == null) return null;
        return volumeSizeInGb.getValue("EndpointConfigurationProductionVariantArgs.volumeSizeInGb");
    }

}
