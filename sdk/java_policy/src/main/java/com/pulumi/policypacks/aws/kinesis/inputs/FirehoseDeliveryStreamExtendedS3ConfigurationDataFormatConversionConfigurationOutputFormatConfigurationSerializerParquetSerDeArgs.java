// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.policypacks.aws.kinesis.inputs;

import com.pulumi.core.UndeferrableValue;
import java.lang.Boolean;
import java.lang.Integer;
import java.lang.String;
import javax.annotation.Nullable;


public final class FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs {

    /**
     * The Hadoop Distributed File System (HDFS) block size. This is useful if you intend to copy the data from Amazon S3 to HDFS before querying. The default is 256 MiB and the minimum is 64 MiB. Kinesis Data Firehose uses this value for padding calculations.
     * 
     */
    private UndeferrableValue<Integer> blockSizeBytes;

    public Integer blockSizeBytes() {
        if (blockSizeBytes == null) return null;
        return blockSizeBytes.getValue("FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs.blockSizeBytes");
    }

    /**
     * The compression code to use over data blocks. The possible values are `UNCOMPRESSED`, `SNAPPY`, and `GZIP`, with the default being `SNAPPY`. Use `SNAPPY` for higher decompression speed. Use `GZIP` if the compression ratio is more important than speed.
     * 
     */
    private UndeferrableValue<String> compression;

    public String compression() {
        if (compression == null) return null;
        return compression.getValue("FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs.compression");
    }

    /**
     * Indicates whether to enable dictionary compression.
     * 
     */
    private UndeferrableValue<Boolean> enableDictionaryCompression;

    public Boolean enableDictionaryCompression() {
        if (enableDictionaryCompression == null) return null;
        return enableDictionaryCompression.getValue("FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs.enableDictionaryCompression");
    }

    /**
     * The maximum amount of padding to apply. This is useful if you intend to copy the data from Amazon S3 to HDFS before querying. The default is `0`.
     * 
     */
    private UndeferrableValue<Integer> maxPaddingBytes;

    public Integer maxPaddingBytes() {
        if (maxPaddingBytes == null) return null;
        return maxPaddingBytes.getValue("FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs.maxPaddingBytes");
    }

    /**
     * The Parquet page size. Column chunks are divided into pages. A page is conceptually an indivisible unit (in terms of compression and encoding). The minimum value is 64 KiB and the default is 1 MiB.
     * 
     */
    private UndeferrableValue<Integer> pageSizeBytes;

    public Integer pageSizeBytes() {
        if (pageSizeBytes == null) return null;
        return pageSizeBytes.getValue("FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs.pageSizeBytes");
    }

    /**
     * Indicates the version of row format to output. The possible values are `V1` and `V2`. The default is `V1`.
     * 
     */
    private UndeferrableValue<String> writerVersion;

    public String writerVersion() {
        if (writerVersion == null) return null;
        return writerVersion.getValue("FirehoseDeliveryStreamExtendedS3ConfigurationDataFormatConversionConfigurationOutputFormatConfigurationSerializerParquetSerDeArgs.writerVersion");
    }

}
