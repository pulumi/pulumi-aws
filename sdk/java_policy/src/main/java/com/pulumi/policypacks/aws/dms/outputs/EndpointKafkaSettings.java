// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.policypacks.aws.dms.outputs;

import com.pulumi.core.UndeferrableValue;
import java.lang.Boolean;
import java.lang.Integer;
import java.lang.String;
import javax.annotation.Nullable;


public final class EndpointKafkaSettings {

    /**
     * Kafka broker location. Specify in the form broker-hostname-or-ip:port.
     * 
     */
    private UndeferrableValue<String> broker;

    public String broker() {
        if (broker == null) return null;
        return broker.getValue("EndpointKafkaSettings.broker");
    }

    /**
     * Shows detailed control information for table definition, column definition, and table and column changes in the Kafka message output. Default is `false`.
     * 
     */
    private @Nullable UndeferrableValue<Boolean> includeControlDetails;

    public @Nullable Boolean includeControlDetails() {
        if (includeControlDetails == null) return null;
        return includeControlDetails.getValue("EndpointKafkaSettings.includeControlDetails");
    }

    /**
     * Include NULL and empty columns for records migrated to the endpoint. Default is `false`.
     * 
     */
    private @Nullable UndeferrableValue<Boolean> includeNullAndEmpty;

    public @Nullable Boolean includeNullAndEmpty() {
        if (includeNullAndEmpty == null) return null;
        return includeNullAndEmpty.getValue("EndpointKafkaSettings.includeNullAndEmpty");
    }

    /**
     * Shows the partition value within the Kafka message output unless the partition type is `schema-table-type`. Default is `false`.
     * 
     */
    private @Nullable UndeferrableValue<Boolean> includePartitionValue;

    public @Nullable Boolean includePartitionValue() {
        if (includePartitionValue == null) return null;
        return includePartitionValue.getValue("EndpointKafkaSettings.includePartitionValue");
    }

    /**
     * Includes any data definition language (DDL) operations that change the table in the control data, such as `rename-table`, `drop-table`, `add-column`, `drop-column`, and `rename-column`. Default is `false`.
     * 
     */
    private @Nullable UndeferrableValue<Boolean> includeTableAlterOperations;

    public @Nullable Boolean includeTableAlterOperations() {
        if (includeTableAlterOperations == null) return null;
        return includeTableAlterOperations.getValue("EndpointKafkaSettings.includeTableAlterOperations");
    }

    /**
     * Provides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for `transaction_id`, previous `transaction_id`, and `transaction_record_id` (the record offset within a transaction). Default is `false`.
     * 
     */
    private @Nullable UndeferrableValue<Boolean> includeTransactionDetails;

    public @Nullable Boolean includeTransactionDetails() {
        if (includeTransactionDetails == null) return null;
        return includeTransactionDetails.getValue("EndpointKafkaSettings.includeTransactionDetails");
    }

    /**
     * Output format for the records created on the endpoint. Message format is `JSON` (default) or `JSON_UNFORMATTED` (a single line with no tab).
     * 
     */
    private @Nullable UndeferrableValue<String> messageFormat;

    public @Nullable String messageFormat() {
        if (messageFormat == null) return null;
        return messageFormat.getValue("EndpointKafkaSettings.messageFormat");
    }

    /**
     * Maximum size in bytes for records created on the endpoint Default is `1,000,000`.
     * 
     */
    private @Nullable UndeferrableValue<Integer> messageMaxBytes;

    public @Nullable Integer messageMaxBytes() {
        if (messageMaxBytes == null) return null;
        return messageMaxBytes.getValue("EndpointKafkaSettings.messageMaxBytes");
    }

    /**
     * Set this optional parameter to true to avoid adding a &#39;0x&#39; prefix to raw data in hexadecimal format. For example, by default, AWS DMS adds a &#39;0x&#39; prefix to the LOB column type in hexadecimal format moving from an Oracle source to a Kafka target. Use the `no_hex_prefix` endpoint setting to enable migration of RAW data type columns without adding the `&#39;0x&#39;` prefix.
     * 
     */
    private @Nullable UndeferrableValue<Boolean> noHexPrefix;

    public @Nullable Boolean noHexPrefix() {
        if (noHexPrefix == null) return null;
        return noHexPrefix.getValue("EndpointKafkaSettings.noHexPrefix");
    }

    /**
     * Prefixes schema and table names to partition values, when the partition type is `primary-key-type`. Doing this increases data distribution among Kafka partitions. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same partition, which causes throttling. Default is `false`.
     * 
     */
    private @Nullable UndeferrableValue<Boolean> partitionIncludeSchemaTable;

    public @Nullable Boolean partitionIncludeSchemaTable() {
        if (partitionIncludeSchemaTable == null) return null;
        return partitionIncludeSchemaTable.getValue("EndpointKafkaSettings.partitionIncludeSchemaTable");
    }

    /**
     * For SASL/SSL authentication, AWS DMS supports the `scram-sha-512` mechanism by default. AWS DMS versions 3.5.0 and later also support the PLAIN mechanism. To use the PLAIN mechanism, set this parameter to `plain`.
     * 
     */
    private @Nullable UndeferrableValue<String> saslMechanism;

    public @Nullable String saslMechanism() {
        if (saslMechanism == null) return null;
        return saslMechanism.getValue("EndpointKafkaSettings.saslMechanism");
    }

    /**
     * Secure password you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.
     * 
     */
    private @Nullable UndeferrableValue<String> saslPassword;

    public @Nullable String saslPassword() {
        if (saslPassword == null) return null;
        return saslPassword.getValue("EndpointKafkaSettings.saslPassword");
    }

    /**
     * Secure user name you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.
     * 
     */
    private @Nullable UndeferrableValue<String> saslUsername;

    public @Nullable String saslUsername() {
        if (saslUsername == null) return null;
        return saslUsername.getValue("EndpointKafkaSettings.saslUsername");
    }

    /**
     * Set secure connection to a Kafka target endpoint using Transport Layer Security (TLS). Options include `ssl-encryption`, `ssl-authentication`, and `sasl-ssl`. `sasl-ssl` requires `sasl_username` and `sasl_password`.
     * 
     */
    private @Nullable UndeferrableValue<String> securityProtocol;

    public @Nullable String securityProtocol() {
        if (securityProtocol == null) return null;
        return securityProtocol.getValue("EndpointKafkaSettings.securityProtocol");
    }

    /**
     * ARN for the private certificate authority (CA) cert that AWS DMS uses to securely connect to your Kafka target endpoint.
     * 
     */
    private @Nullable UndeferrableValue<String> sslCaCertificateArn;

    public @Nullable String sslCaCertificateArn() {
        if (sslCaCertificateArn == null) return null;
        return sslCaCertificateArn.getValue("EndpointKafkaSettings.sslCaCertificateArn");
    }

    /**
     * ARN of the client certificate used to securely connect to a Kafka target endpoint.
     * 
     */
    private @Nullable UndeferrableValue<String> sslClientCertificateArn;

    public @Nullable String sslClientCertificateArn() {
        if (sslClientCertificateArn == null) return null;
        return sslClientCertificateArn.getValue("EndpointKafkaSettings.sslClientCertificateArn");
    }

    /**
     * ARN for the client private key used to securely connect to a Kafka target endpoint.
     * 
     */
    private @Nullable UndeferrableValue<String> sslClientKeyArn;

    public @Nullable String sslClientKeyArn() {
        if (sslClientKeyArn == null) return null;
        return sslClientKeyArn.getValue("EndpointKafkaSettings.sslClientKeyArn");
    }

    /**
     * Password for the client private key used to securely connect to a Kafka target endpoint.
     * 
     */
    private @Nullable UndeferrableValue<String> sslClientKeyPassword;

    public @Nullable String sslClientKeyPassword() {
        if (sslClientKeyPassword == null) return null;
        return sslClientKeyPassword.getValue("EndpointKafkaSettings.sslClientKeyPassword");
    }

    /**
     * Kafka topic for migration. Default is `kafka-default-topic`.
     * 
     */
    private @Nullable UndeferrableValue<String> topic;

    public @Nullable String topic() {
        if (topic == null) return null;
        return topic.getValue("EndpointKafkaSettings.topic");
    }

}
