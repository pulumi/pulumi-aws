// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.policypacks.aws.sagemaker;

import com.pulumi.policypacks.aws.sagemaker.DataQualityJobDefinitionDataQualityJobInputBatchTransformInputDatasetFormat;
import java.lang.String;


public final class DataQualityJobDefinitionDataQualityJobInputBatchTransformInput {

    /**
     * The Amazon S3 location being used to capture the data.
     * 
     */
    public String dataCapturedDestinationS3Uri;



    /**
     * The dataset format for your batch transform job. Fields are documented below.
     * 
     */
    public DataQualityJobDefinitionDataQualityJobInputBatchTransformInputDatasetFormat datasetFormat;



    /**
     * Path to the filesystem where the batch transform data is available to the container. Defaults to `/opt/ml/processing/input`.
     * 
     */
    public String localPath;



    /**
     * Whether input data distributed in Amazon S3 is fully replicated or sharded by an S3 key. Defaults to `FullyReplicated`. Valid values are `FullyReplicated` or `ShardedByS3Key`
     * 
     */
    public String s3DataDistributionType;



    /**
     * Whether the `Pipe` or `File` is used as the input mode for transferring data for the monitoring job. `Pipe` mode is recommended for large datasets. `File` mode is useful for small files that fit in memory. Defaults to `File`.  Valid values are `Pipe` or `File`
     * 
     */
    public String s3InputMode;



}
