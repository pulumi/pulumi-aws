// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package glue

import (
	"context"
	"reflect"

	"github.com/pkg/errors"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

// Manages a Glue Crawler. More information can be found in the [AWS Glue Developer Guide](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html)
//
// ## Example Usage
// ### DynamoDB Target Example
//
// ```go
// package main
//
// import (
// 	"github.com/pulumi/pulumi-aws/sdk/v4/go/aws/glue"
// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
// )
//
// func main() {
// 	pulumi.Run(func(ctx *pulumi.Context) error {
// 		_, err := glue.NewCrawler(ctx, "example", &glue.CrawlerArgs{
// 			DatabaseName: pulumi.Any(aws_glue_catalog_database.Example.Name),
// 			Role:         pulumi.Any(aws_iam_role.Example.Arn),
// 			DynamodbTargets: glue.CrawlerDynamodbTargetArray{
// 				&glue.CrawlerDynamodbTargetArgs{
// 					Path: pulumi.String("table-name"),
// 				},
// 			},
// 		})
// 		if err != nil {
// 			return err
// 		}
// 		return nil
// 	})
// }
// ```
// ### JDBC Target Example
//
// ```go
// package main
//
// import (
// 	"fmt"
//
// 	"github.com/pulumi/pulumi-aws/sdk/v4/go/aws/glue"
// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
// )
//
// func main() {
// 	pulumi.Run(func(ctx *pulumi.Context) error {
// 		_, err := glue.NewCrawler(ctx, "example", &glue.CrawlerArgs{
// 			DatabaseName: pulumi.Any(aws_glue_catalog_database.Example.Name),
// 			Role:         pulumi.Any(aws_iam_role.Example.Arn),
// 			JdbcTargets: glue.CrawlerJdbcTargetArray{
// 				&glue.CrawlerJdbcTargetArgs{
// 					ConnectionName: pulumi.Any(aws_glue_connection.Example.Name),
// 					Path:           pulumi.String(fmt.Sprintf("%v%v", "database-name/", "%")),
// 				},
// 			},
// 		})
// 		if err != nil {
// 			return err
// 		}
// 		return nil
// 	})
// }
// ```
// ### S3 Target Example
//
// ```go
// package main
//
// import (
// 	"fmt"
//
// 	"github.com/pulumi/pulumi-aws/sdk/v4/go/aws/glue"
// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
// )
//
// func main() {
// 	pulumi.Run(func(ctx *pulumi.Context) error {
// 		_, err := glue.NewCrawler(ctx, "example", &glue.CrawlerArgs{
// 			DatabaseName: pulumi.Any(aws_glue_catalog_database.Example.Name),
// 			Role:         pulumi.Any(aws_iam_role.Example.Arn),
// 			S3Targets: glue.CrawlerS3TargetArray{
// 				&glue.CrawlerS3TargetArgs{
// 					Path: pulumi.String(fmt.Sprintf("%v%v", "s3://", aws_s3_bucket.Example.Bucket)),
// 				},
// 			},
// 		})
// 		if err != nil {
// 			return err
// 		}
// 		return nil
// 	})
// }
// ```
// ### Catalog Target Example
//
// ```go
// package main
//
// import (
// 	"fmt"
//
// 	"github.com/pulumi/pulumi-aws/sdk/v4/go/aws/glue"
// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
// )
//
// func main() {
// 	pulumi.Run(func(ctx *pulumi.Context) error {
// 		_, err := glue.NewCrawler(ctx, "example", &glue.CrawlerArgs{
// 			DatabaseName: pulumi.Any(aws_glue_catalog_database.Example.Name),
// 			Role:         pulumi.Any(aws_iam_role.Example.Arn),
// 			CatalogTargets: glue.CrawlerCatalogTargetArray{
// 				&glue.CrawlerCatalogTargetArgs{
// 					DatabaseName: pulumi.Any(aws_glue_catalog_database.Example.Name),
// 					Tables: pulumi.StringArray{
// 						pulumi.Any(aws_glue_catalog_table.Example.Name),
// 					},
// 				},
// 			},
// 			SchemaChangePolicy: &glue.CrawlerSchemaChangePolicyArgs{
// 				DeleteBehavior: pulumi.String("LOG"),
// 			},
// 			Configuration: pulumi.String(fmt.Sprintf("%v%v%v%v%v%v", "{\n", "  \"Version\":1.0,\n", "  \"Grouping\": {\n", "    \"TableGroupingPolicy\": \"CombineCompatibleSchemas\"\n", "  }\n", "}\n")),
// 		})
// 		if err != nil {
// 			return err
// 		}
// 		return nil
// 	})
// }
// ```
// ### MongoDB Target Example
//
// ```go
// package main
//
// import (
// 	"fmt"
//
// 	"github.com/pulumi/pulumi-aws/sdk/v4/go/aws/glue"
// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
// )
//
// func main() {
// 	pulumi.Run(func(ctx *pulumi.Context) error {
// 		_, err := glue.NewCrawler(ctx, "example", &glue.CrawlerArgs{
// 			DatabaseName: pulumi.Any(aws_glue_catalog_database.Example.Name),
// 			Role:         pulumi.Any(aws_iam_role.Example.Arn),
// 			MongodbTargets: glue.CrawlerMongodbTargetArray{
// 				&glue.CrawlerMongodbTargetArgs{
// 					ConnectionName: pulumi.Any(aws_glue_connection.Example.Name),
// 					Path:           pulumi.String(fmt.Sprintf("%v%v", "database-name/", "%")),
// 				},
// 			},
// 		})
// 		if err != nil {
// 			return err
// 		}
// 		return nil
// 	})
// }
// ```
// ### Configuration Settings Example
//
// ```go
// package main
//
// import (
// 	"encoding/json"
// 	"fmt"
//
// 	"github.com/pulumi/pulumi-aws/sdk/v4/go/aws/glue"
// 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
// )
//
// func main() {
// 	pulumi.Run(func(ctx *pulumi.Context) error {
// 		tmpJSON0, err := json.Marshal(map[string]interface{}{
// 			"Grouping": map[string]interface{}{
// 				"TableGroupingPolicy": "CombineCompatibleSchemas",
// 			},
// 			"CrawlerOutput": map[string]interface{}{
// 				"Partitions": map[string]interface{}{
// 					"AddOrUpdateBehavior": "InheritFromTable",
// 				},
// 			},
// 			"Version": 1,
// 		})
// 		if err != nil {
// 			return err
// 		}
// 		json0 := string(tmpJSON0)
// 		_, err := glue.NewCrawler(ctx, "eventsCrawler", &glue.CrawlerArgs{
// 			DatabaseName:  pulumi.Any(aws_glue_catalog_database.Glue_database.Name),
// 			Schedule:      pulumi.String("cron(0 1 * * ? *)"),
// 			Role:          pulumi.Any(aws_iam_role.Glue_role.Arn),
// 			Tags:          pulumi.Any(_var.Tags),
// 			Configuration: pulumi.String(json0),
// 			S3Targets: glue.CrawlerS3TargetArray{
// 				&glue.CrawlerS3TargetArgs{
// 					Path: pulumi.String(fmt.Sprintf("%v%v", "s3://", aws_s3_bucket.Data_lake_bucket.Bucket)),
// 				},
// 			},
// 		})
// 		if err != nil {
// 			return err
// 		}
// 		return nil
// 	})
// }
// ```
//
// ## Import
//
// Glue Crawlers can be imported using `name`, e.g.
//
// ```sh
//  $ pulumi import aws:glue/crawler:Crawler MyJob MyJob
// ```
type Crawler struct {
	pulumi.CustomResourceState

	// The ARN of the crawler
	Arn            pulumi.StringOutput             `pulumi:"arn"`
	CatalogTargets CrawlerCatalogTargetArrayOutput `pulumi:"catalogTargets"`
	// List of custom classifiers. By default, all AWS classifiers are included in a crawl, but these custom classifiers always override the default classifiers for a given classification.
	Classifiers pulumi.StringArrayOutput `pulumi:"classifiers"`
	// JSON string of configuration information. For more details see [Setting Crawler Configuration Options](https://docs.aws.amazon.com/glue/latest/dg/crawler-configuration.html).
	Configuration pulumi.StringPtrOutput `pulumi:"configuration"`
	// The name of the Glue database to be synchronized.
	DatabaseName pulumi.StringOutput `pulumi:"databaseName"`
	// Description of the crawler.
	Description pulumi.StringPtrOutput `pulumi:"description"`
	// List of nested DynamoDB target arguments. See Dynamodb Target below.
	DynamodbTargets CrawlerDynamodbTargetArrayOutput `pulumi:"dynamodbTargets"`
	// List of nested JBDC target arguments. See JDBC Target below.
	JdbcTargets CrawlerJdbcTargetArrayOutput `pulumi:"jdbcTargets"`
	// Specifies data lineage configuration settings for the crawler. See Lineage Configuration below.
	LineageConfiguration CrawlerLineageConfigurationPtrOutput `pulumi:"lineageConfiguration"`
	// List nested MongoDB target arguments. See MongoDB Target below.
	MongodbTargets CrawlerMongodbTargetArrayOutput `pulumi:"mongodbTargets"`
	// Name of the crawler.
	Name pulumi.StringOutput `pulumi:"name"`
	// A policy that specifies whether to crawl the entire dataset again, or to crawl only folders that were added since the last crawler run.. See Recrawl Policy below.
	RecrawlPolicy CrawlerRecrawlPolicyPtrOutput `pulumi:"recrawlPolicy"`
	// The IAM role friendly name (including path without leading slash), or ARN of an IAM role, used by the crawler to access other resources.
	Role pulumi.StringOutput `pulumi:"role"`
	// List nested Amazon S3 target arguments. See S3 Target below.
	S3Targets CrawlerS3TargetArrayOutput `pulumi:"s3Targets"`
	// A cron expression used to specify the schedule. For more information, see [Time-Based Schedules for Jobs and Crawlers](https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html). For example, to run something every day at 12:15 UTC, you would specify: `cron(15 12 * * ? *)`.
	Schedule pulumi.StringPtrOutput `pulumi:"schedule"`
	// Policy for the crawler's update and deletion behavior. See Schema Change Policy below.
	SchemaChangePolicy CrawlerSchemaChangePolicyPtrOutput `pulumi:"schemaChangePolicy"`
	// The name of Security Configuration to be used by the crawler
	SecurityConfiguration pulumi.StringPtrOutput `pulumi:"securityConfiguration"`
	// The table prefix used for catalog tables that are created.
	TablePrefix pulumi.StringPtrOutput `pulumi:"tablePrefix"`
	// Key-value map of resource tags. .If configured with a provider `defaultTags` configuration block present, tags with matching keys will overwrite those defined at the provider-level.
	Tags pulumi.StringMapOutput `pulumi:"tags"`
	// A map of tags assigned to the resource, including those inherited from the provider .
	TagsAll pulumi.StringMapOutput `pulumi:"tagsAll"`
}

// NewCrawler registers a new resource with the given unique name, arguments, and options.
func NewCrawler(ctx *pulumi.Context,
	name string, args *CrawlerArgs, opts ...pulumi.ResourceOption) (*Crawler, error) {
	if args == nil {
		return nil, errors.New("missing one or more required arguments")
	}

	if args.DatabaseName == nil {
		return nil, errors.New("invalid value for required argument 'DatabaseName'")
	}
	if args.Role == nil {
		return nil, errors.New("invalid value for required argument 'Role'")
	}
	var resource Crawler
	err := ctx.RegisterResource("aws:glue/crawler:Crawler", name, args, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetCrawler gets an existing Crawler resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetCrawler(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *CrawlerState, opts ...pulumi.ResourceOption) (*Crawler, error) {
	var resource Crawler
	err := ctx.ReadResource("aws:glue/crawler:Crawler", name, id, state, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering Crawler resources.
type crawlerState struct {
	// The ARN of the crawler
	Arn            *string                `pulumi:"arn"`
	CatalogTargets []CrawlerCatalogTarget `pulumi:"catalogTargets"`
	// List of custom classifiers. By default, all AWS classifiers are included in a crawl, but these custom classifiers always override the default classifiers for a given classification.
	Classifiers []string `pulumi:"classifiers"`
	// JSON string of configuration information. For more details see [Setting Crawler Configuration Options](https://docs.aws.amazon.com/glue/latest/dg/crawler-configuration.html).
	Configuration *string `pulumi:"configuration"`
	// The name of the Glue database to be synchronized.
	DatabaseName *string `pulumi:"databaseName"`
	// Description of the crawler.
	Description *string `pulumi:"description"`
	// List of nested DynamoDB target arguments. See Dynamodb Target below.
	DynamodbTargets []CrawlerDynamodbTarget `pulumi:"dynamodbTargets"`
	// List of nested JBDC target arguments. See JDBC Target below.
	JdbcTargets []CrawlerJdbcTarget `pulumi:"jdbcTargets"`
	// Specifies data lineage configuration settings for the crawler. See Lineage Configuration below.
	LineageConfiguration *CrawlerLineageConfiguration `pulumi:"lineageConfiguration"`
	// List nested MongoDB target arguments. See MongoDB Target below.
	MongodbTargets []CrawlerMongodbTarget `pulumi:"mongodbTargets"`
	// Name of the crawler.
	Name *string `pulumi:"name"`
	// A policy that specifies whether to crawl the entire dataset again, or to crawl only folders that were added since the last crawler run.. See Recrawl Policy below.
	RecrawlPolicy *CrawlerRecrawlPolicy `pulumi:"recrawlPolicy"`
	// The IAM role friendly name (including path without leading slash), or ARN of an IAM role, used by the crawler to access other resources.
	Role *string `pulumi:"role"`
	// List nested Amazon S3 target arguments. See S3 Target below.
	S3Targets []CrawlerS3Target `pulumi:"s3Targets"`
	// A cron expression used to specify the schedule. For more information, see [Time-Based Schedules for Jobs and Crawlers](https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html). For example, to run something every day at 12:15 UTC, you would specify: `cron(15 12 * * ? *)`.
	Schedule *string `pulumi:"schedule"`
	// Policy for the crawler's update and deletion behavior. See Schema Change Policy below.
	SchemaChangePolicy *CrawlerSchemaChangePolicy `pulumi:"schemaChangePolicy"`
	// The name of Security Configuration to be used by the crawler
	SecurityConfiguration *string `pulumi:"securityConfiguration"`
	// The table prefix used for catalog tables that are created.
	TablePrefix *string `pulumi:"tablePrefix"`
	// Key-value map of resource tags. .If configured with a provider `defaultTags` configuration block present, tags with matching keys will overwrite those defined at the provider-level.
	Tags map[string]string `pulumi:"tags"`
	// A map of tags assigned to the resource, including those inherited from the provider .
	TagsAll map[string]string `pulumi:"tagsAll"`
}

type CrawlerState struct {
	// The ARN of the crawler
	Arn            pulumi.StringPtrInput
	CatalogTargets CrawlerCatalogTargetArrayInput
	// List of custom classifiers. By default, all AWS classifiers are included in a crawl, but these custom classifiers always override the default classifiers for a given classification.
	Classifiers pulumi.StringArrayInput
	// JSON string of configuration information. For more details see [Setting Crawler Configuration Options](https://docs.aws.amazon.com/glue/latest/dg/crawler-configuration.html).
	Configuration pulumi.StringPtrInput
	// The name of the Glue database to be synchronized.
	DatabaseName pulumi.StringPtrInput
	// Description of the crawler.
	Description pulumi.StringPtrInput
	// List of nested DynamoDB target arguments. See Dynamodb Target below.
	DynamodbTargets CrawlerDynamodbTargetArrayInput
	// List of nested JBDC target arguments. See JDBC Target below.
	JdbcTargets CrawlerJdbcTargetArrayInput
	// Specifies data lineage configuration settings for the crawler. See Lineage Configuration below.
	LineageConfiguration CrawlerLineageConfigurationPtrInput
	// List nested MongoDB target arguments. See MongoDB Target below.
	MongodbTargets CrawlerMongodbTargetArrayInput
	// Name of the crawler.
	Name pulumi.StringPtrInput
	// A policy that specifies whether to crawl the entire dataset again, or to crawl only folders that were added since the last crawler run.. See Recrawl Policy below.
	RecrawlPolicy CrawlerRecrawlPolicyPtrInput
	// The IAM role friendly name (including path without leading slash), or ARN of an IAM role, used by the crawler to access other resources.
	Role pulumi.StringPtrInput
	// List nested Amazon S3 target arguments. See S3 Target below.
	S3Targets CrawlerS3TargetArrayInput
	// A cron expression used to specify the schedule. For more information, see [Time-Based Schedules for Jobs and Crawlers](https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html). For example, to run something every day at 12:15 UTC, you would specify: `cron(15 12 * * ? *)`.
	Schedule pulumi.StringPtrInput
	// Policy for the crawler's update and deletion behavior. See Schema Change Policy below.
	SchemaChangePolicy CrawlerSchemaChangePolicyPtrInput
	// The name of Security Configuration to be used by the crawler
	SecurityConfiguration pulumi.StringPtrInput
	// The table prefix used for catalog tables that are created.
	TablePrefix pulumi.StringPtrInput
	// Key-value map of resource tags. .If configured with a provider `defaultTags` configuration block present, tags with matching keys will overwrite those defined at the provider-level.
	Tags pulumi.StringMapInput
	// A map of tags assigned to the resource, including those inherited from the provider .
	TagsAll pulumi.StringMapInput
}

func (CrawlerState) ElementType() reflect.Type {
	return reflect.TypeOf((*crawlerState)(nil)).Elem()
}

type crawlerArgs struct {
	CatalogTargets []CrawlerCatalogTarget `pulumi:"catalogTargets"`
	// List of custom classifiers. By default, all AWS classifiers are included in a crawl, but these custom classifiers always override the default classifiers for a given classification.
	Classifiers []string `pulumi:"classifiers"`
	// JSON string of configuration information. For more details see [Setting Crawler Configuration Options](https://docs.aws.amazon.com/glue/latest/dg/crawler-configuration.html).
	Configuration *string `pulumi:"configuration"`
	// The name of the Glue database to be synchronized.
	DatabaseName string `pulumi:"databaseName"`
	// Description of the crawler.
	Description *string `pulumi:"description"`
	// List of nested DynamoDB target arguments. See Dynamodb Target below.
	DynamodbTargets []CrawlerDynamodbTarget `pulumi:"dynamodbTargets"`
	// List of nested JBDC target arguments. See JDBC Target below.
	JdbcTargets []CrawlerJdbcTarget `pulumi:"jdbcTargets"`
	// Specifies data lineage configuration settings for the crawler. See Lineage Configuration below.
	LineageConfiguration *CrawlerLineageConfiguration `pulumi:"lineageConfiguration"`
	// List nested MongoDB target arguments. See MongoDB Target below.
	MongodbTargets []CrawlerMongodbTarget `pulumi:"mongodbTargets"`
	// Name of the crawler.
	Name *string `pulumi:"name"`
	// A policy that specifies whether to crawl the entire dataset again, or to crawl only folders that were added since the last crawler run.. See Recrawl Policy below.
	RecrawlPolicy *CrawlerRecrawlPolicy `pulumi:"recrawlPolicy"`
	// The IAM role friendly name (including path without leading slash), or ARN of an IAM role, used by the crawler to access other resources.
	Role string `pulumi:"role"`
	// List nested Amazon S3 target arguments. See S3 Target below.
	S3Targets []CrawlerS3Target `pulumi:"s3Targets"`
	// A cron expression used to specify the schedule. For more information, see [Time-Based Schedules for Jobs and Crawlers](https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html). For example, to run something every day at 12:15 UTC, you would specify: `cron(15 12 * * ? *)`.
	Schedule *string `pulumi:"schedule"`
	// Policy for the crawler's update and deletion behavior. See Schema Change Policy below.
	SchemaChangePolicy *CrawlerSchemaChangePolicy `pulumi:"schemaChangePolicy"`
	// The name of Security Configuration to be used by the crawler
	SecurityConfiguration *string `pulumi:"securityConfiguration"`
	// The table prefix used for catalog tables that are created.
	TablePrefix *string `pulumi:"tablePrefix"`
	// Key-value map of resource tags. .If configured with a provider `defaultTags` configuration block present, tags with matching keys will overwrite those defined at the provider-level.
	Tags map[string]string `pulumi:"tags"`
	// A map of tags assigned to the resource, including those inherited from the provider .
	TagsAll map[string]string `pulumi:"tagsAll"`
}

// The set of arguments for constructing a Crawler resource.
type CrawlerArgs struct {
	CatalogTargets CrawlerCatalogTargetArrayInput
	// List of custom classifiers. By default, all AWS classifiers are included in a crawl, but these custom classifiers always override the default classifiers for a given classification.
	Classifiers pulumi.StringArrayInput
	// JSON string of configuration information. For more details see [Setting Crawler Configuration Options](https://docs.aws.amazon.com/glue/latest/dg/crawler-configuration.html).
	Configuration pulumi.StringPtrInput
	// The name of the Glue database to be synchronized.
	DatabaseName pulumi.StringInput
	// Description of the crawler.
	Description pulumi.StringPtrInput
	// List of nested DynamoDB target arguments. See Dynamodb Target below.
	DynamodbTargets CrawlerDynamodbTargetArrayInput
	// List of nested JBDC target arguments. See JDBC Target below.
	JdbcTargets CrawlerJdbcTargetArrayInput
	// Specifies data lineage configuration settings for the crawler. See Lineage Configuration below.
	LineageConfiguration CrawlerLineageConfigurationPtrInput
	// List nested MongoDB target arguments. See MongoDB Target below.
	MongodbTargets CrawlerMongodbTargetArrayInput
	// Name of the crawler.
	Name pulumi.StringPtrInput
	// A policy that specifies whether to crawl the entire dataset again, or to crawl only folders that were added since the last crawler run.. See Recrawl Policy below.
	RecrawlPolicy CrawlerRecrawlPolicyPtrInput
	// The IAM role friendly name (including path without leading slash), or ARN of an IAM role, used by the crawler to access other resources.
	Role pulumi.StringInput
	// List nested Amazon S3 target arguments. See S3 Target below.
	S3Targets CrawlerS3TargetArrayInput
	// A cron expression used to specify the schedule. For more information, see [Time-Based Schedules for Jobs and Crawlers](https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html). For example, to run something every day at 12:15 UTC, you would specify: `cron(15 12 * * ? *)`.
	Schedule pulumi.StringPtrInput
	// Policy for the crawler's update and deletion behavior. See Schema Change Policy below.
	SchemaChangePolicy CrawlerSchemaChangePolicyPtrInput
	// The name of Security Configuration to be used by the crawler
	SecurityConfiguration pulumi.StringPtrInput
	// The table prefix used for catalog tables that are created.
	TablePrefix pulumi.StringPtrInput
	// Key-value map of resource tags. .If configured with a provider `defaultTags` configuration block present, tags with matching keys will overwrite those defined at the provider-level.
	Tags pulumi.StringMapInput
	// A map of tags assigned to the resource, including those inherited from the provider .
	TagsAll pulumi.StringMapInput
}

func (CrawlerArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*crawlerArgs)(nil)).Elem()
}

type CrawlerInput interface {
	pulumi.Input

	ToCrawlerOutput() CrawlerOutput
	ToCrawlerOutputWithContext(ctx context.Context) CrawlerOutput
}

func (*Crawler) ElementType() reflect.Type {
	return reflect.TypeOf((*Crawler)(nil))
}

func (i *Crawler) ToCrawlerOutput() CrawlerOutput {
	return i.ToCrawlerOutputWithContext(context.Background())
}

func (i *Crawler) ToCrawlerOutputWithContext(ctx context.Context) CrawlerOutput {
	return pulumi.ToOutputWithContext(ctx, i).(CrawlerOutput)
}

func (i *Crawler) ToCrawlerPtrOutput() CrawlerPtrOutput {
	return i.ToCrawlerPtrOutputWithContext(context.Background())
}

func (i *Crawler) ToCrawlerPtrOutputWithContext(ctx context.Context) CrawlerPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(CrawlerPtrOutput)
}

type CrawlerPtrInput interface {
	pulumi.Input

	ToCrawlerPtrOutput() CrawlerPtrOutput
	ToCrawlerPtrOutputWithContext(ctx context.Context) CrawlerPtrOutput
}

type crawlerPtrType CrawlerArgs

func (*crawlerPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**Crawler)(nil))
}

func (i *crawlerPtrType) ToCrawlerPtrOutput() CrawlerPtrOutput {
	return i.ToCrawlerPtrOutputWithContext(context.Background())
}

func (i *crawlerPtrType) ToCrawlerPtrOutputWithContext(ctx context.Context) CrawlerPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(CrawlerPtrOutput)
}

// CrawlerArrayInput is an input type that accepts CrawlerArray and CrawlerArrayOutput values.
// You can construct a concrete instance of `CrawlerArrayInput` via:
//
//          CrawlerArray{ CrawlerArgs{...} }
type CrawlerArrayInput interface {
	pulumi.Input

	ToCrawlerArrayOutput() CrawlerArrayOutput
	ToCrawlerArrayOutputWithContext(context.Context) CrawlerArrayOutput
}

type CrawlerArray []CrawlerInput

func (CrawlerArray) ElementType() reflect.Type {
	return reflect.TypeOf(([]*Crawler)(nil))
}

func (i CrawlerArray) ToCrawlerArrayOutput() CrawlerArrayOutput {
	return i.ToCrawlerArrayOutputWithContext(context.Background())
}

func (i CrawlerArray) ToCrawlerArrayOutputWithContext(ctx context.Context) CrawlerArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(CrawlerArrayOutput)
}

// CrawlerMapInput is an input type that accepts CrawlerMap and CrawlerMapOutput values.
// You can construct a concrete instance of `CrawlerMapInput` via:
//
//          CrawlerMap{ "key": CrawlerArgs{...} }
type CrawlerMapInput interface {
	pulumi.Input

	ToCrawlerMapOutput() CrawlerMapOutput
	ToCrawlerMapOutputWithContext(context.Context) CrawlerMapOutput
}

type CrawlerMap map[string]CrawlerInput

func (CrawlerMap) ElementType() reflect.Type {
	return reflect.TypeOf((map[string]*Crawler)(nil))
}

func (i CrawlerMap) ToCrawlerMapOutput() CrawlerMapOutput {
	return i.ToCrawlerMapOutputWithContext(context.Background())
}

func (i CrawlerMap) ToCrawlerMapOutputWithContext(ctx context.Context) CrawlerMapOutput {
	return pulumi.ToOutputWithContext(ctx, i).(CrawlerMapOutput)
}

type CrawlerOutput struct {
	*pulumi.OutputState
}

func (CrawlerOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*Crawler)(nil))
}

func (o CrawlerOutput) ToCrawlerOutput() CrawlerOutput {
	return o
}

func (o CrawlerOutput) ToCrawlerOutputWithContext(ctx context.Context) CrawlerOutput {
	return o
}

func (o CrawlerOutput) ToCrawlerPtrOutput() CrawlerPtrOutput {
	return o.ToCrawlerPtrOutputWithContext(context.Background())
}

func (o CrawlerOutput) ToCrawlerPtrOutputWithContext(ctx context.Context) CrawlerPtrOutput {
	return o.ApplyT(func(v Crawler) *Crawler {
		return &v
	}).(CrawlerPtrOutput)
}

type CrawlerPtrOutput struct {
	*pulumi.OutputState
}

func (CrawlerPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**Crawler)(nil))
}

func (o CrawlerPtrOutput) ToCrawlerPtrOutput() CrawlerPtrOutput {
	return o
}

func (o CrawlerPtrOutput) ToCrawlerPtrOutputWithContext(ctx context.Context) CrawlerPtrOutput {
	return o
}

type CrawlerArrayOutput struct{ *pulumi.OutputState }

func (CrawlerArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]Crawler)(nil))
}

func (o CrawlerArrayOutput) ToCrawlerArrayOutput() CrawlerArrayOutput {
	return o
}

func (o CrawlerArrayOutput) ToCrawlerArrayOutputWithContext(ctx context.Context) CrawlerArrayOutput {
	return o
}

func (o CrawlerArrayOutput) Index(i pulumi.IntInput) CrawlerOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) Crawler {
		return vs[0].([]Crawler)[vs[1].(int)]
	}).(CrawlerOutput)
}

type CrawlerMapOutput struct{ *pulumi.OutputState }

func (CrawlerMapOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]Crawler)(nil))
}

func (o CrawlerMapOutput) ToCrawlerMapOutput() CrawlerMapOutput {
	return o
}

func (o CrawlerMapOutput) ToCrawlerMapOutputWithContext(ctx context.Context) CrawlerMapOutput {
	return o
}

func (o CrawlerMapOutput) MapIndex(k pulumi.StringInput) CrawlerOutput {
	return pulumi.All(o, k).ApplyT(func(vs []interface{}) Crawler {
		return vs[0].(map[string]Crawler)[vs[1].(string)]
	}).(CrawlerOutput)
}

func init() {
	pulumi.RegisterOutputType(CrawlerOutput{})
	pulumi.RegisterOutputType(CrawlerPtrOutput{})
	pulumi.RegisterOutputType(CrawlerArrayOutput{})
	pulumi.RegisterOutputType(CrawlerMapOutput{})
}
