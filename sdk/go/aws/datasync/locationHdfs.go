// Code generated by the Pulumi Terraform Bridge (tfgen) Tool DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package datasync

import (
	"context"
	"reflect"

	"errors"
	"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/internal"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

// Manages an HDFS Location within AWS DataSync.
//
// > **NOTE:** The DataSync Agents must be available before creating this resource.
//
// ## Example Usage
//
// ```go
// package main
//
// import (
//
//	"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/datasync"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			_, err := datasync.NewLocationHdfs(ctx, "example", &datasync.LocationHdfsArgs{
//				AgentArns: pulumi.StringArray{
//					aws_datasync_agent.Example.Arn,
//				},
//				AuthenticationType: pulumi.String("SIMPLE"),
//				SimpleUser:         pulumi.String("example"),
//				NameNodes: datasync.LocationHdfsNameNodeArray{
//					&datasync.LocationHdfsNameNodeArgs{
//						Hostname: pulumi.Any(aws_instance.Example.Private_dns),
//						Port:     pulumi.Int(80),
//					},
//				},
//			})
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
//
// ## Import
//
// terraform import {
//
//	to = aws_datasync_location_hdfs.example
//
//	id = "arn:aws:datasync:us-east-1:123456789012:location/loc-12345678901234567" } Using `pulumi import`, import `aws_datasync_location_hdfs` using the Amazon Resource Name (ARN). For exampleconsole % pulumi import aws_datasync_location_hdfs.example arn:aws:datasync:us-east-1:123456789012:location/loc-12345678901234567
type LocationHdfs struct {
	pulumi.CustomResourceState

	// A list of DataSync Agent ARNs with which this location will be associated.
	AgentArns pulumi.StringArrayOutput `pulumi:"agentArns"`
	// Amazon Resource Name (ARN) of the DataSync Location.
	Arn pulumi.StringOutput `pulumi:"arn"`
	// The type of authentication used to determine the identity of the user. Valid values are `SIMPLE` and `KERBEROS`.
	AuthenticationType pulumi.StringPtrOutput `pulumi:"authenticationType"`
	// The size of data blocks to write into the HDFS cluster. The block size must be a multiple of 512 bytes. The default block size is 128 mebibytes (MiB).
	BlockSize pulumi.IntPtrOutput `pulumi:"blockSize"`
	// The Kerberos key table (keytab) that contains mappings between the defined Kerberos principal and the encrypted keys. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
	KerberosKeytab pulumi.StringPtrOutput `pulumi:"kerberosKeytab"`
	// The krb5.conf file that contains the Kerberos configuration information. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
	KerberosKrb5Conf pulumi.StringPtrOutput `pulumi:"kerberosKrb5Conf"`
	// The Kerberos principal with access to the files and folders on the HDFS cluster. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
	KerberosPrincipal pulumi.StringPtrOutput `pulumi:"kerberosPrincipal"`
	// The URI of the HDFS cluster's Key Management Server (KMS).
	KmsKeyProviderUri pulumi.StringPtrOutput `pulumi:"kmsKeyProviderUri"`
	// The NameNode that manages the HDFS namespace. The NameNode performs operations such as opening, closing, and renaming files and directories. The NameNode contains the information to map blocks of data to the DataNodes. You can use only one NameNode. See configuration below.
	NameNodes LocationHdfsNameNodeArrayOutput `pulumi:"nameNodes"`
	// The Quality of Protection (QOP) configuration specifies the Remote Procedure Call (RPC) and data transfer protection settings configured on the Hadoop Distributed File System (HDFS) cluster. If `qopConfiguration` isn't specified, `rpcProtection` and `dataTransferProtection` default to `PRIVACY`. If you set RpcProtection or DataTransferProtection, the other parameter assumes the same value.  See configuration below.
	QopConfiguration LocationHdfsQopConfigurationPtrOutput `pulumi:"qopConfiguration"`
	// The number of DataNodes to replicate the data to when writing to the HDFS cluster. By default, data is replicated to three DataNodes.
	ReplicationFactor pulumi.IntPtrOutput `pulumi:"replicationFactor"`
	// The user name used to identify the client on the host operating system. If `SIMPLE` is specified for `authenticationType`, this parameter is required.
	SimpleUser pulumi.StringPtrOutput `pulumi:"simpleUser"`
	// A subdirectory in the HDFS cluster. This subdirectory is used to read data from or write data to the HDFS cluster. If the subdirectory isn't specified, it will default to /.
	Subdirectory pulumi.StringPtrOutput `pulumi:"subdirectory"`
	// Key-value pairs of resource tags to assign to the DataSync Location. If configured with a provider `defaultTags` configuration block present, tags with matching keys will overwrite those defined at the provider-level.
	Tags pulumi.StringMapOutput `pulumi:"tags"`
	// A map of tags assigned to the resource, including those inherited from the provider `defaultTags` configuration block.
	TagsAll pulumi.StringMapOutput `pulumi:"tagsAll"`
	Uri     pulumi.StringOutput    `pulumi:"uri"`
}

// NewLocationHdfs registers a new resource with the given unique name, arguments, and options.
func NewLocationHdfs(ctx *pulumi.Context,
	name string, args *LocationHdfsArgs, opts ...pulumi.ResourceOption) (*LocationHdfs, error) {
	if args == nil {
		return nil, errors.New("missing one or more required arguments")
	}

	if args.AgentArns == nil {
		return nil, errors.New("invalid value for required argument 'AgentArns'")
	}
	if args.NameNodes == nil {
		return nil, errors.New("invalid value for required argument 'NameNodes'")
	}
	opts = internal.PkgResourceDefaultOpts(opts)
	var resource LocationHdfs
	err := ctx.RegisterResource("aws:datasync/locationHdfs:LocationHdfs", name, args, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetLocationHdfs gets an existing LocationHdfs resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetLocationHdfs(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *LocationHdfsState, opts ...pulumi.ResourceOption) (*LocationHdfs, error) {
	var resource LocationHdfs
	err := ctx.ReadResource("aws:datasync/locationHdfs:LocationHdfs", name, id, state, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering LocationHdfs resources.
type locationHdfsState struct {
	// A list of DataSync Agent ARNs with which this location will be associated.
	AgentArns []string `pulumi:"agentArns"`
	// Amazon Resource Name (ARN) of the DataSync Location.
	Arn *string `pulumi:"arn"`
	// The type of authentication used to determine the identity of the user. Valid values are `SIMPLE` and `KERBEROS`.
	AuthenticationType *string `pulumi:"authenticationType"`
	// The size of data blocks to write into the HDFS cluster. The block size must be a multiple of 512 bytes. The default block size is 128 mebibytes (MiB).
	BlockSize *int `pulumi:"blockSize"`
	// The Kerberos key table (keytab) that contains mappings between the defined Kerberos principal and the encrypted keys. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
	KerberosKeytab *string `pulumi:"kerberosKeytab"`
	// The krb5.conf file that contains the Kerberos configuration information. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
	KerberosKrb5Conf *string `pulumi:"kerberosKrb5Conf"`
	// The Kerberos principal with access to the files and folders on the HDFS cluster. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
	KerberosPrincipal *string `pulumi:"kerberosPrincipal"`
	// The URI of the HDFS cluster's Key Management Server (KMS).
	KmsKeyProviderUri *string `pulumi:"kmsKeyProviderUri"`
	// The NameNode that manages the HDFS namespace. The NameNode performs operations such as opening, closing, and renaming files and directories. The NameNode contains the information to map blocks of data to the DataNodes. You can use only one NameNode. See configuration below.
	NameNodes []LocationHdfsNameNode `pulumi:"nameNodes"`
	// The Quality of Protection (QOP) configuration specifies the Remote Procedure Call (RPC) and data transfer protection settings configured on the Hadoop Distributed File System (HDFS) cluster. If `qopConfiguration` isn't specified, `rpcProtection` and `dataTransferProtection` default to `PRIVACY`. If you set RpcProtection or DataTransferProtection, the other parameter assumes the same value.  See configuration below.
	QopConfiguration *LocationHdfsQopConfiguration `pulumi:"qopConfiguration"`
	// The number of DataNodes to replicate the data to when writing to the HDFS cluster. By default, data is replicated to three DataNodes.
	ReplicationFactor *int `pulumi:"replicationFactor"`
	// The user name used to identify the client on the host operating system. If `SIMPLE` is specified for `authenticationType`, this parameter is required.
	SimpleUser *string `pulumi:"simpleUser"`
	// A subdirectory in the HDFS cluster. This subdirectory is used to read data from or write data to the HDFS cluster. If the subdirectory isn't specified, it will default to /.
	Subdirectory *string `pulumi:"subdirectory"`
	// Key-value pairs of resource tags to assign to the DataSync Location. If configured with a provider `defaultTags` configuration block present, tags with matching keys will overwrite those defined at the provider-level.
	Tags map[string]string `pulumi:"tags"`
	// A map of tags assigned to the resource, including those inherited from the provider `defaultTags` configuration block.
	TagsAll map[string]string `pulumi:"tagsAll"`
	Uri     *string           `pulumi:"uri"`
}

type LocationHdfsState struct {
	// A list of DataSync Agent ARNs with which this location will be associated.
	AgentArns pulumi.StringArrayInput
	// Amazon Resource Name (ARN) of the DataSync Location.
	Arn pulumi.StringPtrInput
	// The type of authentication used to determine the identity of the user. Valid values are `SIMPLE` and `KERBEROS`.
	AuthenticationType pulumi.StringPtrInput
	// The size of data blocks to write into the HDFS cluster. The block size must be a multiple of 512 bytes. The default block size is 128 mebibytes (MiB).
	BlockSize pulumi.IntPtrInput
	// The Kerberos key table (keytab) that contains mappings between the defined Kerberos principal and the encrypted keys. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
	KerberosKeytab pulumi.StringPtrInput
	// The krb5.conf file that contains the Kerberos configuration information. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
	KerberosKrb5Conf pulumi.StringPtrInput
	// The Kerberos principal with access to the files and folders on the HDFS cluster. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
	KerberosPrincipal pulumi.StringPtrInput
	// The URI of the HDFS cluster's Key Management Server (KMS).
	KmsKeyProviderUri pulumi.StringPtrInput
	// The NameNode that manages the HDFS namespace. The NameNode performs operations such as opening, closing, and renaming files and directories. The NameNode contains the information to map blocks of data to the DataNodes. You can use only one NameNode. See configuration below.
	NameNodes LocationHdfsNameNodeArrayInput
	// The Quality of Protection (QOP) configuration specifies the Remote Procedure Call (RPC) and data transfer protection settings configured on the Hadoop Distributed File System (HDFS) cluster. If `qopConfiguration` isn't specified, `rpcProtection` and `dataTransferProtection` default to `PRIVACY`. If you set RpcProtection or DataTransferProtection, the other parameter assumes the same value.  See configuration below.
	QopConfiguration LocationHdfsQopConfigurationPtrInput
	// The number of DataNodes to replicate the data to when writing to the HDFS cluster. By default, data is replicated to three DataNodes.
	ReplicationFactor pulumi.IntPtrInput
	// The user name used to identify the client on the host operating system. If `SIMPLE` is specified for `authenticationType`, this parameter is required.
	SimpleUser pulumi.StringPtrInput
	// A subdirectory in the HDFS cluster. This subdirectory is used to read data from or write data to the HDFS cluster. If the subdirectory isn't specified, it will default to /.
	Subdirectory pulumi.StringPtrInput
	// Key-value pairs of resource tags to assign to the DataSync Location. If configured with a provider `defaultTags` configuration block present, tags with matching keys will overwrite those defined at the provider-level.
	Tags pulumi.StringMapInput
	// A map of tags assigned to the resource, including those inherited from the provider `defaultTags` configuration block.
	TagsAll pulumi.StringMapInput
	Uri     pulumi.StringPtrInput
}

func (LocationHdfsState) ElementType() reflect.Type {
	return reflect.TypeOf((*locationHdfsState)(nil)).Elem()
}

type locationHdfsArgs struct {
	// A list of DataSync Agent ARNs with which this location will be associated.
	AgentArns []string `pulumi:"agentArns"`
	// The type of authentication used to determine the identity of the user. Valid values are `SIMPLE` and `KERBEROS`.
	AuthenticationType *string `pulumi:"authenticationType"`
	// The size of data blocks to write into the HDFS cluster. The block size must be a multiple of 512 bytes. The default block size is 128 mebibytes (MiB).
	BlockSize *int `pulumi:"blockSize"`
	// The Kerberos key table (keytab) that contains mappings between the defined Kerberos principal and the encrypted keys. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
	KerberosKeytab *string `pulumi:"kerberosKeytab"`
	// The krb5.conf file that contains the Kerberos configuration information. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
	KerberosKrb5Conf *string `pulumi:"kerberosKrb5Conf"`
	// The Kerberos principal with access to the files and folders on the HDFS cluster. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
	KerberosPrincipal *string `pulumi:"kerberosPrincipal"`
	// The URI of the HDFS cluster's Key Management Server (KMS).
	KmsKeyProviderUri *string `pulumi:"kmsKeyProviderUri"`
	// The NameNode that manages the HDFS namespace. The NameNode performs operations such as opening, closing, and renaming files and directories. The NameNode contains the information to map blocks of data to the DataNodes. You can use only one NameNode. See configuration below.
	NameNodes []LocationHdfsNameNode `pulumi:"nameNodes"`
	// The Quality of Protection (QOP) configuration specifies the Remote Procedure Call (RPC) and data transfer protection settings configured on the Hadoop Distributed File System (HDFS) cluster. If `qopConfiguration` isn't specified, `rpcProtection` and `dataTransferProtection` default to `PRIVACY`. If you set RpcProtection or DataTransferProtection, the other parameter assumes the same value.  See configuration below.
	QopConfiguration *LocationHdfsQopConfiguration `pulumi:"qopConfiguration"`
	// The number of DataNodes to replicate the data to when writing to the HDFS cluster. By default, data is replicated to three DataNodes.
	ReplicationFactor *int `pulumi:"replicationFactor"`
	// The user name used to identify the client on the host operating system. If `SIMPLE` is specified for `authenticationType`, this parameter is required.
	SimpleUser *string `pulumi:"simpleUser"`
	// A subdirectory in the HDFS cluster. This subdirectory is used to read data from or write data to the HDFS cluster. If the subdirectory isn't specified, it will default to /.
	Subdirectory *string `pulumi:"subdirectory"`
	// Key-value pairs of resource tags to assign to the DataSync Location. If configured with a provider `defaultTags` configuration block present, tags with matching keys will overwrite those defined at the provider-level.
	Tags map[string]string `pulumi:"tags"`
}

// The set of arguments for constructing a LocationHdfs resource.
type LocationHdfsArgs struct {
	// A list of DataSync Agent ARNs with which this location will be associated.
	AgentArns pulumi.StringArrayInput
	// The type of authentication used to determine the identity of the user. Valid values are `SIMPLE` and `KERBEROS`.
	AuthenticationType pulumi.StringPtrInput
	// The size of data blocks to write into the HDFS cluster. The block size must be a multiple of 512 bytes. The default block size is 128 mebibytes (MiB).
	BlockSize pulumi.IntPtrInput
	// The Kerberos key table (keytab) that contains mappings between the defined Kerberos principal and the encrypted keys. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
	KerberosKeytab pulumi.StringPtrInput
	// The krb5.conf file that contains the Kerberos configuration information. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
	KerberosKrb5Conf pulumi.StringPtrInput
	// The Kerberos principal with access to the files and folders on the HDFS cluster. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
	KerberosPrincipal pulumi.StringPtrInput
	// The URI of the HDFS cluster's Key Management Server (KMS).
	KmsKeyProviderUri pulumi.StringPtrInput
	// The NameNode that manages the HDFS namespace. The NameNode performs operations such as opening, closing, and renaming files and directories. The NameNode contains the information to map blocks of data to the DataNodes. You can use only one NameNode. See configuration below.
	NameNodes LocationHdfsNameNodeArrayInput
	// The Quality of Protection (QOP) configuration specifies the Remote Procedure Call (RPC) and data transfer protection settings configured on the Hadoop Distributed File System (HDFS) cluster. If `qopConfiguration` isn't specified, `rpcProtection` and `dataTransferProtection` default to `PRIVACY`. If you set RpcProtection or DataTransferProtection, the other parameter assumes the same value.  See configuration below.
	QopConfiguration LocationHdfsQopConfigurationPtrInput
	// The number of DataNodes to replicate the data to when writing to the HDFS cluster. By default, data is replicated to three DataNodes.
	ReplicationFactor pulumi.IntPtrInput
	// The user name used to identify the client on the host operating system. If `SIMPLE` is specified for `authenticationType`, this parameter is required.
	SimpleUser pulumi.StringPtrInput
	// A subdirectory in the HDFS cluster. This subdirectory is used to read data from or write data to the HDFS cluster. If the subdirectory isn't specified, it will default to /.
	Subdirectory pulumi.StringPtrInput
	// Key-value pairs of resource tags to assign to the DataSync Location. If configured with a provider `defaultTags` configuration block present, tags with matching keys will overwrite those defined at the provider-level.
	Tags pulumi.StringMapInput
}

func (LocationHdfsArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*locationHdfsArgs)(nil)).Elem()
}

type LocationHdfsInput interface {
	pulumi.Input

	ToLocationHdfsOutput() LocationHdfsOutput
	ToLocationHdfsOutputWithContext(ctx context.Context) LocationHdfsOutput
}

func (*LocationHdfs) ElementType() reflect.Type {
	return reflect.TypeOf((**LocationHdfs)(nil)).Elem()
}

func (i *LocationHdfs) ToLocationHdfsOutput() LocationHdfsOutput {
	return i.ToLocationHdfsOutputWithContext(context.Background())
}

func (i *LocationHdfs) ToLocationHdfsOutputWithContext(ctx context.Context) LocationHdfsOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LocationHdfsOutput)
}

// LocationHdfsArrayInput is an input type that accepts LocationHdfsArray and LocationHdfsArrayOutput values.
// You can construct a concrete instance of `LocationHdfsArrayInput` via:
//
//	LocationHdfsArray{ LocationHdfsArgs{...} }
type LocationHdfsArrayInput interface {
	pulumi.Input

	ToLocationHdfsArrayOutput() LocationHdfsArrayOutput
	ToLocationHdfsArrayOutputWithContext(context.Context) LocationHdfsArrayOutput
}

type LocationHdfsArray []LocationHdfsInput

func (LocationHdfsArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*LocationHdfs)(nil)).Elem()
}

func (i LocationHdfsArray) ToLocationHdfsArrayOutput() LocationHdfsArrayOutput {
	return i.ToLocationHdfsArrayOutputWithContext(context.Background())
}

func (i LocationHdfsArray) ToLocationHdfsArrayOutputWithContext(ctx context.Context) LocationHdfsArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LocationHdfsArrayOutput)
}

// LocationHdfsMapInput is an input type that accepts LocationHdfsMap and LocationHdfsMapOutput values.
// You can construct a concrete instance of `LocationHdfsMapInput` via:
//
//	LocationHdfsMap{ "key": LocationHdfsArgs{...} }
type LocationHdfsMapInput interface {
	pulumi.Input

	ToLocationHdfsMapOutput() LocationHdfsMapOutput
	ToLocationHdfsMapOutputWithContext(context.Context) LocationHdfsMapOutput
}

type LocationHdfsMap map[string]LocationHdfsInput

func (LocationHdfsMap) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*LocationHdfs)(nil)).Elem()
}

func (i LocationHdfsMap) ToLocationHdfsMapOutput() LocationHdfsMapOutput {
	return i.ToLocationHdfsMapOutputWithContext(context.Background())
}

func (i LocationHdfsMap) ToLocationHdfsMapOutputWithContext(ctx context.Context) LocationHdfsMapOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LocationHdfsMapOutput)
}

type LocationHdfsOutput struct{ *pulumi.OutputState }

func (LocationHdfsOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**LocationHdfs)(nil)).Elem()
}

func (o LocationHdfsOutput) ToLocationHdfsOutput() LocationHdfsOutput {
	return o
}

func (o LocationHdfsOutput) ToLocationHdfsOutputWithContext(ctx context.Context) LocationHdfsOutput {
	return o
}

// A list of DataSync Agent ARNs with which this location will be associated.
func (o LocationHdfsOutput) AgentArns() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *LocationHdfs) pulumi.StringArrayOutput { return v.AgentArns }).(pulumi.StringArrayOutput)
}

// Amazon Resource Name (ARN) of the DataSync Location.
func (o LocationHdfsOutput) Arn() pulumi.StringOutput {
	return o.ApplyT(func(v *LocationHdfs) pulumi.StringOutput { return v.Arn }).(pulumi.StringOutput)
}

// The type of authentication used to determine the identity of the user. Valid values are `SIMPLE` and `KERBEROS`.
func (o LocationHdfsOutput) AuthenticationType() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *LocationHdfs) pulumi.StringPtrOutput { return v.AuthenticationType }).(pulumi.StringPtrOutput)
}

// The size of data blocks to write into the HDFS cluster. The block size must be a multiple of 512 bytes. The default block size is 128 mebibytes (MiB).
func (o LocationHdfsOutput) BlockSize() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *LocationHdfs) pulumi.IntPtrOutput { return v.BlockSize }).(pulumi.IntPtrOutput)
}

// The Kerberos key table (keytab) that contains mappings between the defined Kerberos principal and the encrypted keys. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
func (o LocationHdfsOutput) KerberosKeytab() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *LocationHdfs) pulumi.StringPtrOutput { return v.KerberosKeytab }).(pulumi.StringPtrOutput)
}

// The krb5.conf file that contains the Kerberos configuration information. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
func (o LocationHdfsOutput) KerberosKrb5Conf() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *LocationHdfs) pulumi.StringPtrOutput { return v.KerberosKrb5Conf }).(pulumi.StringPtrOutput)
}

// The Kerberos principal with access to the files and folders on the HDFS cluster. If `KERBEROS` is specified for `authenticationType`, this parameter is required.
func (o LocationHdfsOutput) KerberosPrincipal() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *LocationHdfs) pulumi.StringPtrOutput { return v.KerberosPrincipal }).(pulumi.StringPtrOutput)
}

// The URI of the HDFS cluster's Key Management Server (KMS).
func (o LocationHdfsOutput) KmsKeyProviderUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *LocationHdfs) pulumi.StringPtrOutput { return v.KmsKeyProviderUri }).(pulumi.StringPtrOutput)
}

// The NameNode that manages the HDFS namespace. The NameNode performs operations such as opening, closing, and renaming files and directories. The NameNode contains the information to map blocks of data to the DataNodes. You can use only one NameNode. See configuration below.
func (o LocationHdfsOutput) NameNodes() LocationHdfsNameNodeArrayOutput {
	return o.ApplyT(func(v *LocationHdfs) LocationHdfsNameNodeArrayOutput { return v.NameNodes }).(LocationHdfsNameNodeArrayOutput)
}

// The Quality of Protection (QOP) configuration specifies the Remote Procedure Call (RPC) and data transfer protection settings configured on the Hadoop Distributed File System (HDFS) cluster. If `qopConfiguration` isn't specified, `rpcProtection` and `dataTransferProtection` default to `PRIVACY`. If you set RpcProtection or DataTransferProtection, the other parameter assumes the same value.  See configuration below.
func (o LocationHdfsOutput) QopConfiguration() LocationHdfsQopConfigurationPtrOutput {
	return o.ApplyT(func(v *LocationHdfs) LocationHdfsQopConfigurationPtrOutput { return v.QopConfiguration }).(LocationHdfsQopConfigurationPtrOutput)
}

// The number of DataNodes to replicate the data to when writing to the HDFS cluster. By default, data is replicated to three DataNodes.
func (o LocationHdfsOutput) ReplicationFactor() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *LocationHdfs) pulumi.IntPtrOutput { return v.ReplicationFactor }).(pulumi.IntPtrOutput)
}

// The user name used to identify the client on the host operating system. If `SIMPLE` is specified for `authenticationType`, this parameter is required.
func (o LocationHdfsOutput) SimpleUser() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *LocationHdfs) pulumi.StringPtrOutput { return v.SimpleUser }).(pulumi.StringPtrOutput)
}

// A subdirectory in the HDFS cluster. This subdirectory is used to read data from or write data to the HDFS cluster. If the subdirectory isn't specified, it will default to /.
func (o LocationHdfsOutput) Subdirectory() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *LocationHdfs) pulumi.StringPtrOutput { return v.Subdirectory }).(pulumi.StringPtrOutput)
}

// Key-value pairs of resource tags to assign to the DataSync Location. If configured with a provider `defaultTags` configuration block present, tags with matching keys will overwrite those defined at the provider-level.
func (o LocationHdfsOutput) Tags() pulumi.StringMapOutput {
	return o.ApplyT(func(v *LocationHdfs) pulumi.StringMapOutput { return v.Tags }).(pulumi.StringMapOutput)
}

// A map of tags assigned to the resource, including those inherited from the provider `defaultTags` configuration block.
func (o LocationHdfsOutput) TagsAll() pulumi.StringMapOutput {
	return o.ApplyT(func(v *LocationHdfs) pulumi.StringMapOutput { return v.TagsAll }).(pulumi.StringMapOutput)
}

func (o LocationHdfsOutput) Uri() pulumi.StringOutput {
	return o.ApplyT(func(v *LocationHdfs) pulumi.StringOutput { return v.Uri }).(pulumi.StringOutput)
}

type LocationHdfsArrayOutput struct{ *pulumi.OutputState }

func (LocationHdfsArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*LocationHdfs)(nil)).Elem()
}

func (o LocationHdfsArrayOutput) ToLocationHdfsArrayOutput() LocationHdfsArrayOutput {
	return o
}

func (o LocationHdfsArrayOutput) ToLocationHdfsArrayOutputWithContext(ctx context.Context) LocationHdfsArrayOutput {
	return o
}

func (o LocationHdfsArrayOutput) Index(i pulumi.IntInput) LocationHdfsOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) *LocationHdfs {
		return vs[0].([]*LocationHdfs)[vs[1].(int)]
	}).(LocationHdfsOutput)
}

type LocationHdfsMapOutput struct{ *pulumi.OutputState }

func (LocationHdfsMapOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*LocationHdfs)(nil)).Elem()
}

func (o LocationHdfsMapOutput) ToLocationHdfsMapOutput() LocationHdfsMapOutput {
	return o
}

func (o LocationHdfsMapOutput) ToLocationHdfsMapOutputWithContext(ctx context.Context) LocationHdfsMapOutput {
	return o
}

func (o LocationHdfsMapOutput) MapIndex(k pulumi.StringInput) LocationHdfsOutput {
	return pulumi.All(o, k).ApplyT(func(vs []interface{}) *LocationHdfs {
		return vs[0].(map[string]*LocationHdfs)[vs[1].(string)]
	}).(LocationHdfsOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*LocationHdfsInput)(nil)).Elem(), &LocationHdfs{})
	pulumi.RegisterInputType(reflect.TypeOf((*LocationHdfsArrayInput)(nil)).Elem(), LocationHdfsArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*LocationHdfsMapInput)(nil)).Elem(), LocationHdfsMap{})
	pulumi.RegisterOutputType(LocationHdfsOutput{})
	pulumi.RegisterOutputType(LocationHdfsArrayOutput{})
	pulumi.RegisterOutputType(LocationHdfsMapOutput{})
}
