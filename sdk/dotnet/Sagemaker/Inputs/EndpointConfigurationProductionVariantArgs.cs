// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Aws.Sagemaker.Inputs
{

    public sealed class EndpointConfigurationProductionVariantArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// The size of the Elastic Inference (EI) instance to use for the production variant.
        /// </summary>
        [Input("acceleratorType")]
        public Input<string>? AcceleratorType { get; set; }

        /// <summary>
        /// The timeout value, in seconds, for your inference container to pass health check by SageMaker AI Hosting. For more information about health check, see [How Your Container Should Respond to Health Check (Ping) Requests](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-algo-ping-requests). Valid values between `60` and `3600`.
        /// </summary>
        [Input("containerStartupHealthCheckTimeoutInSeconds")]
        public Input<int>? ContainerStartupHealthCheckTimeoutInSeconds { get; set; }

        /// <summary>
        /// Specifies configuration for a core dump from the model container when the process crashes. Fields are documented below.
        /// </summary>
        [Input("coreDumpConfig")]
        public Input<Inputs.EndpointConfigurationProductionVariantCoreDumpConfigArgs>? CoreDumpConfig { get; set; }

        /// <summary>
        /// You can use this parameter to turn on native Amazon Web Services Systems Manager (SSM) access for a production variant behind an endpoint. By default, SSM access is disabled for all production variants behind an endpoints.
        /// </summary>
        [Input("enableSsmAccess")]
        public Input<bool>? EnableSsmAccess { get; set; }

        /// <summary>
        /// Specifies an option from a collection of preconfigured Amazon Machine Image (AMI) images. Each image is configured by Amazon Web Services with a set of software and driver versions. Amazon Web Services optimizes these configurations for different machine learning workloads.
        /// </summary>
        [Input("inferenceAmiVersion")]
        public Input<string>? InferenceAmiVersion { get; set; }

        /// <summary>
        /// Initial number of instances used for auto-scaling.
        /// </summary>
        [Input("initialInstanceCount")]
        public Input<int>? InitialInstanceCount { get; set; }

        /// <summary>
        /// Determines initial traffic distribution among all of the models that you specify in the endpoint configuration. If unspecified, it defaults to `1.0`.
        /// </summary>
        [Input("initialVariantWeight")]
        public Input<double>? InitialVariantWeight { get; set; }

        /// <summary>
        /// The type of instance to start.
        /// </summary>
        [Input("instanceType")]
        public Input<string>? InstanceType { get; set; }

        /// <summary>
        /// Settings that control the range in the number of instances that the endpoint provisions as it scales up or down to accommodate traffic.
        /// </summary>
        [Input("managedInstanceScaling")]
        public Input<Inputs.EndpointConfigurationProductionVariantManagedInstanceScalingArgs>? ManagedInstanceScaling { get; set; }

        /// <summary>
        /// The timeout value, in seconds, to download and extract the model that you want to host from Amazon S3 to the individual inference instance associated with this production variant. Valid values between `60` and `3600`.
        /// </summary>
        [Input("modelDataDownloadTimeoutInSeconds")]
        public Input<int>? ModelDataDownloadTimeoutInSeconds { get; set; }

        /// <summary>
        /// The name of the model to use.
        /// </summary>
        [Input("modelName", required: true)]
        public Input<string> ModelName { get; set; } = null!;

        [Input("routingConfigs")]
        private InputList<Inputs.EndpointConfigurationProductionVariantRoutingConfigArgs>? _routingConfigs;

        /// <summary>
        /// Sets how the endpoint routes incoming traffic. See RoutingConfig below.
        /// </summary>
        public InputList<Inputs.EndpointConfigurationProductionVariantRoutingConfigArgs> RoutingConfigs
        {
            get => _routingConfigs ?? (_routingConfigs = new InputList<Inputs.EndpointConfigurationProductionVariantRoutingConfigArgs>());
            set => _routingConfigs = value;
        }

        /// <summary>
        /// Specifies configuration for how an endpoint performs asynchronous inference.
        /// </summary>
        [Input("serverlessConfig")]
        public Input<Inputs.EndpointConfigurationProductionVariantServerlessConfigArgs>? ServerlessConfig { get; set; }

        /// <summary>
        /// The name of the variant. If omitted, this provider will assign a random, unique name.
        /// </summary>
        [Input("variantName")]
        public Input<string>? VariantName { get; set; }

        /// <summary>
        /// The size, in GB, of the ML storage volume attached to individual inference instance associated with the production variant. Valid values between `1` and `512`.
        /// </summary>
        [Input("volumeSizeInGb")]
        public Input<int>? VolumeSizeInGb { get; set; }

        public EndpointConfigurationProductionVariantArgs()
        {
        }
        public static new EndpointConfigurationProductionVariantArgs Empty => new EndpointConfigurationProductionVariantArgs();
    }
}
