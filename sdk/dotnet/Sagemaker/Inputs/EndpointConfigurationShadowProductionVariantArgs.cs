// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Aws.Sagemaker.Inputs
{

    public sealed class EndpointConfigurationShadowProductionVariantArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// Size of the Elastic Inference (EI) instance to use for the production variant.
        /// </summary>
        [Input("acceleratorType")]
        public Input<string>? AcceleratorType { get; set; }

        /// <summary>
        /// Timeout value, in seconds, for your inference container to pass health check by SageMaker AI Hosting. For more information about health check, see [How Your Container Should Respond to Health Check (Ping) Requests](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-algo-ping-requests). Valid values between `60` and `3600`.
        /// </summary>
        [Input("containerStartupHealthCheckTimeoutInSeconds")]
        public Input<int>? ContainerStartupHealthCheckTimeoutInSeconds { get; set; }

        /// <summary>
        /// Core dump configuration from the model container when the process crashes. Fields are documented below.
        /// </summary>
        [Input("coreDumpConfig")]
        public Input<Inputs.EndpointConfigurationShadowProductionVariantCoreDumpConfigArgs>? CoreDumpConfig { get; set; }

        /// <summary>
        /// Whether to turn on native AWS SSM access for a production variant behind an endpoint. By default, SSM access is disabled for all production variants behind endpoints. Ignored if `ModelName` is not set (Inference Components endpoint).
        /// </summary>
        [Input("enableSsmAccess")]
        public Input<bool>? EnableSsmAccess { get; set; }

        /// <summary>
        /// Option from a collection of preconfigured AMI images. Each image is configured by AWS with a set of software and driver versions. AWS optimizes these configurations for different machine learning workloads.
        /// </summary>
        [Input("inferenceAmiVersion")]
        public Input<string>? InferenceAmiVersion { get; set; }

        /// <summary>
        /// Initial number of instances used for auto-scaling.
        /// </summary>
        [Input("initialInstanceCount")]
        public Input<int>? InitialInstanceCount { get; set; }

        /// <summary>
        /// Initial traffic distribution among all of the models that you specify in the endpoint configuration. If unspecified, defaults to `1.0`. Ignored if `ModelName` is not set (Inference Components endpoint).
        /// </summary>
        [Input("initialVariantWeight")]
        public Input<double>? InitialVariantWeight { get; set; }

        /// <summary>
        /// Type of instance to start.
        /// </summary>
        [Input("instanceType")]
        public Input<string>? InstanceType { get; set; }

        /// <summary>
        /// Control the range in the number of instances that the endpoint provisions as it scales up or down to accommodate traffic.
        /// </summary>
        [Input("managedInstanceScaling")]
        public Input<Inputs.EndpointConfigurationShadowProductionVariantManagedInstanceScalingArgs>? ManagedInstanceScaling { get; set; }

        /// <summary>
        /// Timeout value, in seconds, to download and extract the model that you want to host from S3 to the individual inference instance associated with this production variant. Valid values between `60` and `3600`.
        /// </summary>
        [Input("modelDataDownloadTimeoutInSeconds")]
        public Input<int>? ModelDataDownloadTimeoutInSeconds { get; set; }

        /// <summary>
        /// Name of the model to use. Required unless using Inference Components (in which case `ExecutionRoleArn` must be specified at the endpoint configuration level).
        /// </summary>
        [Input("modelName")]
        public Input<string>? ModelName { get; set; }

        [Input("routingConfigs")]
        private InputList<Inputs.EndpointConfigurationShadowProductionVariantRoutingConfigArgs>? _routingConfigs;

        /// <summary>
        /// How the endpoint routes incoming traffic. See RoutingConfig below.
        /// </summary>
        public InputList<Inputs.EndpointConfigurationShadowProductionVariantRoutingConfigArgs> RoutingConfigs
        {
            get => _routingConfigs ?? (_routingConfigs = new InputList<Inputs.EndpointConfigurationShadowProductionVariantRoutingConfigArgs>());
            set => _routingConfigs = value;
        }

        /// <summary>
        /// How an endpoint performs asynchronous inference.
        /// </summary>
        [Input("serverlessConfig")]
        public Input<Inputs.EndpointConfigurationShadowProductionVariantServerlessConfigArgs>? ServerlessConfig { get; set; }

        /// <summary>
        /// Name of the variant. If omitted, the provider will assign a random, unique name.
        /// </summary>
        [Input("variantName")]
        public Input<string>? VariantName { get; set; }

        /// <summary>
        /// Size, in GB, of the ML storage volume attached to individual inference instance associated with the production variant. Valid values between `1` and `512`.
        /// </summary>
        [Input("volumeSizeInGb")]
        public Input<int>? VolumeSizeInGb { get; set; }

        public EndpointConfigurationShadowProductionVariantArgs()
        {
        }
        public static new EndpointConfigurationShadowProductionVariantArgs Empty => new EndpointConfigurationShadowProductionVariantArgs();
    }
}
