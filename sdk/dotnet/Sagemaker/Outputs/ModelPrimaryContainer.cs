// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Aws.Sagemaker.Outputs
{

    [OutputType]
    public sealed class ModelPrimaryContainer
    {
        /// <summary>
        /// The DNS host name for the container.
        /// </summary>
        public readonly string? ContainerHostname;
        /// <summary>
        /// Environment variables for the Docker container.
        /// A list of key value pairs.
        /// </summary>
        public readonly ImmutableDictionary<string, string>? Environment;
        /// <summary>
        /// The registry path where the inference code image is stored in Amazon ECR.
        /// </summary>
        public readonly string? Image;
        /// <summary>
        /// Specifies whether the model container is in Amazon ECR or a private Docker registry accessible from your Amazon Virtual Private Cloud (VPC). For more information see [Using a Private Docker Registry for Real-Time Inference Containers](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-containers-inference-private.html). see Image Config.
        /// </summary>
        public readonly Outputs.ModelPrimaryContainerImageConfig? ImageConfig;
        /// <summary>
        /// The inference specification name in the model package version.
        /// </summary>
        public readonly string? InferenceSpecificationName;
        /// <summary>
        /// The container hosts value `SingleModel/MultiModel`. The default value is `SingleModel`.
        /// </summary>
        public readonly string? Mode;
        /// <summary>
        /// The location of model data to deploy. Use this for uncompressed model deployment. For information about how to deploy an uncompressed model, see [Deploying uncompressed models](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html) in the _AWS SageMaker AI Developer Guide_.
        /// </summary>
        public readonly Outputs.ModelPrimaryContainerModelDataSource? ModelDataSource;
        /// <summary>
        /// The URL for the S3 location where model artifacts are stored.
        /// </summary>
        public readonly string? ModelDataUrl;
        /// <summary>
        /// The Amazon Resource Name (ARN) of the model package to use to create the model.
        /// </summary>
        public readonly string? ModelPackageName;
        /// <summary>
        /// Specifies additional configuration for multi-model endpoints. see Multi Model Config.
        /// </summary>
        public readonly Outputs.ModelPrimaryContainerMultiModelConfig? MultiModelConfig;

        [OutputConstructor]
        private ModelPrimaryContainer(
            string? containerHostname,

            ImmutableDictionary<string, string>? environment,

            string? image,

            Outputs.ModelPrimaryContainerImageConfig? imageConfig,

            string? inferenceSpecificationName,

            string? mode,

            Outputs.ModelPrimaryContainerModelDataSource? modelDataSource,

            string? modelDataUrl,

            string? modelPackageName,

            Outputs.ModelPrimaryContainerMultiModelConfig? multiModelConfig)
        {
            ContainerHostname = containerHostname;
            Environment = environment;
            Image = image;
            ImageConfig = imageConfig;
            InferenceSpecificationName = inferenceSpecificationName;
            Mode = mode;
            ModelDataSource = modelDataSource;
            ModelDataUrl = modelDataUrl;
            ModelPackageName = modelPackageName;
            MultiModelConfig = multiModelConfig;
        }
    }
}
