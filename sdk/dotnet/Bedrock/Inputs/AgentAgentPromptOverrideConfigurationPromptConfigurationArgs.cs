// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Aws.Bedrock.Inputs
{

    public sealed class AgentAgentPromptOverrideConfigurationPromptConfigurationArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// prompt template with which to replace the default prompt template. You can use placeholder variables in the base prompt template to customize the prompt. For more information, see [Prompt template placeholder variables](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-placeholders.html).
        /// </summary>
        [Input("basePromptTemplate", required: true)]
        public Input<string> BasePromptTemplate { get; set; } = null!;

        [Input("inferenceConfigurations", required: true)]
        private InputList<Inputs.AgentAgentPromptOverrideConfigurationPromptConfigurationInferenceConfigurationArgs>? _inferenceConfigurations;

        /// <summary>
        /// Inference parameters to use when the agent invokes a foundation model in the part of the agent sequence defined by the `prompt_type`. For more information, see [Inference parameters for foundation models](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html). See `inference_configuration` Block for details.
        /// </summary>
        public InputList<Inputs.AgentAgentPromptOverrideConfigurationPromptConfigurationInferenceConfigurationArgs> InferenceConfigurations
        {
            get => _inferenceConfigurations ?? (_inferenceConfigurations = new InputList<Inputs.AgentAgentPromptOverrideConfigurationPromptConfigurationInferenceConfigurationArgs>());
            set => _inferenceConfigurations = value;
        }

        /// <summary>
        /// Whether to override the default parser Lambda function when parsing the raw foundation model output in the part of the agent sequence defined by the `prompt_type`. If you set the argument as `OVERRIDDEN`, the `override_lambda` argument in the `prompt_override_configuration` block must be specified with the ARN of a Lambda function. Valid values: `DEFAULT`, `OVERRIDDEN`.
        /// </summary>
        [Input("parserMode", required: true)]
        public Input<string> ParserMode { get; set; } = null!;

        /// <summary>
        /// Whether to override the default prompt template for this `prompt_type`. Set this argument to `OVERRIDDEN` to use the prompt that you provide in the `base_prompt_template`. If you leave it as `DEFAULT`, the agent uses a default prompt template. Valid values: `DEFAULT`, `OVERRIDDEN`.
        /// </summary>
        [Input("promptCreationMode", required: true)]
        public Input<string> PromptCreationMode { get; set; } = null!;

        /// <summary>
        /// Whether to allow the agent to carry out the step specified in the `prompt_type`. If you set this argument to `DISABLED`, the agent skips that step. Valid Values: `ENABLED`, `DISABLED`.
        /// </summary>
        [Input("promptState", required: true)]
        public Input<string> PromptState { get; set; } = null!;

        /// <summary>
        /// Step in the agent sequence that this prompt configuration applies to. Valid values: `PRE_PROCESSING`, `ORCHESTRATION`, `POST_PROCESSING`, `KNOWLEDGE_BASE_RESPONSE_GENERATION`.
        /// </summary>
        [Input("promptType", required: true)]
        public Input<string> PromptType { get; set; } = null!;

        public AgentAgentPromptOverrideConfigurationPromptConfigurationArgs()
        {
        }
        public static new AgentAgentPromptOverrideConfigurationPromptConfigurationArgs Empty => new AgentAgentPromptOverrideConfigurationPromptConfigurationArgs();
    }
}
