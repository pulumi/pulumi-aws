// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Aws.Bedrock.Inputs
{

    public sealed class AgentPromptVariantArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// Contains model-specific inference configurations that arenâ€™t in the inferenceConfiguration field. To see model-specific inference parameters, see [Inference request parameters and response fields for foundation models](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html).
        /// </summary>
        [Input("additionalModelRequestFields")]
        public Input<string>? AdditionalModelRequestFields { get; set; }

        /// <summary>
        /// Specifies a generative AI resource with which to use the prompt. If this is not supplied, then a `gen_ai_resource` must be defined. See Generative AI Resource for more information.
        /// </summary>
        [Input("genAiResource")]
        public Input<Inputs.AgentPromptVariantGenAiResourceArgs>? GenAiResource { get; set; }

        /// <summary>
        /// Contains inference configurations for the prompt variant. See Inference Configuration for more information.
        /// </summary>
        [Input("inferenceConfiguration")]
        public Input<Inputs.AgentPromptVariantInferenceConfigurationArgs>? InferenceConfiguration { get; set; }

        [Input("metadatas")]
        private InputList<Inputs.AgentPromptVariantMetadataArgs>? _metadatas;

        /// <summary>
        /// A list of objects, each containing a key-value pair that defines a metadata tag and value to attach to a prompt variant. See Metadata for more information.
        /// </summary>
        public InputList<Inputs.AgentPromptVariantMetadataArgs> Metadatas
        {
            get => _metadatas ?? (_metadatas = new InputList<Inputs.AgentPromptVariantMetadataArgs>());
            set => _metadatas = value;
        }

        /// <summary>
        /// Unique identifier of the model or [inference profile](https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference.html) with which to run inference on the prompt. If this is not supplied, then a `gen_ai_resource` must be defined.
        /// </summary>
        [Input("modelId")]
        public Input<string>? ModelId { get; set; }

        /// <summary>
        /// Name of the prompt variant.
        /// </summary>
        [Input("name", required: true)]
        public Input<string> Name { get; set; } = null!;

        /// <summary>
        /// Contains configurations for the prompt template. See Template Configuration for more information.
        /// </summary>
        [Input("templateConfiguration")]
        public Input<Inputs.AgentPromptVariantTemplateConfigurationArgs>? TemplateConfiguration { get; set; }

        /// <summary>
        /// Type of prompt template to use. Valid values: `CHAT`, `TEXT`.
        /// </summary>
        [Input("templateType", required: true)]
        public Input<string> TemplateType { get; set; } = null!;

        public AgentPromptVariantArgs()
        {
        }
        public static new AgentPromptVariantArgs Empty => new AgentPromptVariantArgs();
    }
}
