// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Aws.Bedrock.Inputs
{

    public sealed class AgentPromptVariantInferenceConfigurationTextGetArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// Maximum number of tokens to return in the response.
        /// </summary>
        [Input("maxTokens")]
        public Input<int>? MaxTokens { get; set; }

        [Input("stopSequences")]
        private InputList<string>? _stopSequences;

        /// <summary>
        /// List of strings that define sequences after which the model will stop generating.
        /// </summary>
        public InputList<string> StopSequences
        {
            get => _stopSequences ?? (_stopSequences = new InputList<string>());
            set => _stopSequences = value;
        }

        /// <summary>
        /// Controls the randomness of the response. Choose a lower value for more predictable outputs and a higher value for more surprising outputs.
        /// </summary>
        [Input("temperature")]
        public Input<double>? Temperature { get; set; }

        /// <summary>
        /// Percentage of most-likely candidates that the model considers for the next token.
        /// </summary>
        [Input("topP")]
        public Input<double>? TopP { get; set; }

        public AgentPromptVariantInferenceConfigurationTextGetArgs()
        {
        }
        public static new AgentPromptVariantInferenceConfigurationTextGetArgs Empty => new AgentPromptVariantInferenceConfigurationTextGetArgs();
    }
}
