// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Aws.Bedrock.Inputs
{

    public sealed class AgentAgentPromptOverrideConfigurationPromptConfigurationInferenceConfigurationArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// Maximum number of tokens to allow in the generated response.
        /// </summary>
        [Input("maxLength", required: true)]
        public Input<int> MaxLength { get; set; } = null!;

        [Input("stopSequences", required: true)]
        private InputList<string>? _stopSequences;

        /// <summary>
        /// List of stop sequences. A stop sequence is a sequence of characters that causes the model to stop generating the response.
        /// </summary>
        public InputList<string> StopSequences
        {
            get => _stopSequences ?? (_stopSequences = new InputList<string>());
            set => _stopSequences = value;
        }

        /// <summary>
        /// Likelihood of the model selecting higher-probability options while generating a response. A lower value makes the model more likely to choose higher-probability options, while a higher value makes the model more likely to choose lower-probability options.
        /// </summary>
        [Input("temperature", required: true)]
        public Input<double> Temperature { get; set; } = null!;

        /// <summary>
        /// Number of top most-likely candidates, between 0 and 500, from which the model chooses the next token in the sequence.
        /// </summary>
        [Input("topK", required: true)]
        public Input<int> TopK { get; set; } = null!;

        /// <summary>
        /// Top percentage of the probability distribution of next tokens, between 0 and 1 (denoting 0% and 100%), from which the model chooses the next token in the sequence.
        /// </summary>
        [Input("topP", required: true)]
        public Input<double> TopP { get; set; } = null!;

        public AgentAgentPromptOverrideConfigurationPromptConfigurationInferenceConfigurationArgs()
        {
        }
        public static new AgentAgentPromptOverrideConfigurationPromptConfigurationInferenceConfigurationArgs Empty => new AgentAgentPromptOverrideConfigurationPromptConfigurationInferenceConfigurationArgs();
    }
}
