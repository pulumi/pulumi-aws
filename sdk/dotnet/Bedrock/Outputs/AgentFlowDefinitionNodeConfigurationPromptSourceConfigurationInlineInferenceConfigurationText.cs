// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Aws.Bedrock.Outputs
{

    [OutputType]
    public sealed class AgentFlowDefinitionNodeConfigurationPromptSourceConfigurationInlineInferenceConfigurationText
    {
        /// <summary>
        /// Maximum number of tokens to return in the response.
        /// </summary>
        public readonly int? MaxTokens;
        /// <summary>
        /// List of strings that define sequences after which the model will stop generating.
        /// </summary>
        public readonly ImmutableArray<string> StopSequences;
        /// <summary>
        /// Controls the randomness of the response. Choose a lower value for more predictable outputs and a higher value for more surprising outputs.
        /// </summary>
        public readonly double? Temperature;
        /// <summary>
        /// Percentage of most-likely candidates that the model considers for the next token.
        /// </summary>
        public readonly double? TopP;

        [OutputConstructor]
        private AgentFlowDefinitionNodeConfigurationPromptSourceConfigurationInlineInferenceConfigurationText(
            int? maxTokens,

            ImmutableArray<string> stopSequences,

            double? temperature,

            double? topP)
        {
            MaxTokens = maxTokens;
            StopSequences = stopSequences;
            Temperature = temperature;
            TopP = topP;
        }
    }
}
