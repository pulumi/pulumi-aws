// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Aws.Dms.Inputs
{

    public sealed class EndpointKafkaSettingsArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// Kafka broker location. Specify in the form broker-hostname-or-ip:port.
        /// </summary>
        [Input("broker", required: true)]
        public Input<string> Broker { get; set; } = null!;

        /// <summary>
        /// Shows detailed control information for table definition, column definition, and table and column changes in the Kafka message output. Default is `false`.
        /// </summary>
        [Input("includeControlDetails")]
        public Input<bool>? IncludeControlDetails { get; set; }

        /// <summary>
        /// Include NULL and empty columns for records migrated to the endpoint. Default is `false`.
        /// </summary>
        [Input("includeNullAndEmpty")]
        public Input<bool>? IncludeNullAndEmpty { get; set; }

        /// <summary>
        /// Shows the partition value within the Kafka message output unless the partition type is `schema-table-type`. Default is `false`.
        /// </summary>
        [Input("includePartitionValue")]
        public Input<bool>? IncludePartitionValue { get; set; }

        /// <summary>
        /// Includes any data definition language (DDL) operations that change the table in the control data, such as `rename-table`, `drop-table`, `add-column`, `drop-column`, and `rename-column`. Default is `false`.
        /// </summary>
        [Input("includeTableAlterOperations")]
        public Input<bool>? IncludeTableAlterOperations { get; set; }

        /// <summary>
        /// Provides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for `transaction_id`, previous `transaction_id`, and `transaction_record_id` (the record offset within a transaction). Default is `false`.
        /// </summary>
        [Input("includeTransactionDetails")]
        public Input<bool>? IncludeTransactionDetails { get; set; }

        /// <summary>
        /// Output format for the records created on the endpoint. Message format is `JSON` (default) or `JSON_UNFORMATTED` (a single line with no tab).
        /// </summary>
        [Input("messageFormat")]
        public Input<string>? MessageFormat { get; set; }

        /// <summary>
        /// Maximum size in bytes for records created on the endpoint Default is `1,000,000`.
        /// </summary>
        [Input("messageMaxBytes")]
        public Input<int>? MessageMaxBytes { get; set; }

        /// <summary>
        /// Set this optional parameter to true to avoid adding a '0x' prefix to raw data in hexadecimal format. For example, by default, AWS DMS adds a '0x' prefix to the LOB column type in hexadecimal format moving from an Oracle source to a Kafka target. Use the `no_hex_prefix` endpoint setting to enable migration of RAW data type columns without adding the `'0x'` prefix.
        /// </summary>
        [Input("noHexPrefix")]
        public Input<bool>? NoHexPrefix { get; set; }

        /// <summary>
        /// Prefixes schema and table names to partition values, when the partition type is `primary-key-type`. Doing this increases data distribution among Kafka partitions. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same partition, which causes throttling. Default is `false`.
        /// </summary>
        [Input("partitionIncludeSchemaTable")]
        public Input<bool>? PartitionIncludeSchemaTable { get; set; }

        /// <summary>
        /// For SASL/SSL authentication, AWS DMS supports the `scram-sha-512` mechanism by default. AWS DMS versions 3.5.0 and later also support the PLAIN mechanism. To use the PLAIN mechanism, set this parameter to `plain`.
        /// </summary>
        [Input("saslMechanism")]
        public Input<string>? SaslMechanism { get; set; }

        [Input("saslPassword")]
        private Input<string>? _saslPassword;

        /// <summary>
        /// Secure password you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.
        /// </summary>
        public Input<string>? SaslPassword
        {
            get => _saslPassword;
            set
            {
                var emptySecret = Output.CreateSecret(0);
                _saslPassword = Output.Tuple<Input<string>?, int>(value, emptySecret).Apply(t => t.Item1);
            }
        }

        /// <summary>
        /// Secure user name you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.
        /// </summary>
        [Input("saslUsername")]
        public Input<string>? SaslUsername { get; set; }

        /// <summary>
        /// Set secure connection to a Kafka target endpoint using Transport Layer Security (TLS). Options include `ssl-encryption`, `ssl-authentication`, and `sasl-ssl`. `sasl-ssl` requires `sasl_username` and `sasl_password`.
        /// </summary>
        [Input("securityProtocol")]
        public Input<string>? SecurityProtocol { get; set; }

        /// <summary>
        /// ARN for the private certificate authority (CA) cert that AWS DMS uses to securely connect to your Kafka target endpoint.
        /// </summary>
        [Input("sslCaCertificateArn")]
        public Input<string>? SslCaCertificateArn { get; set; }

        /// <summary>
        /// ARN of the client certificate used to securely connect to a Kafka target endpoint.
        /// </summary>
        [Input("sslClientCertificateArn")]
        public Input<string>? SslClientCertificateArn { get; set; }

        /// <summary>
        /// ARN for the client private key used to securely connect to a Kafka target endpoint.
        /// </summary>
        [Input("sslClientKeyArn")]
        public Input<string>? SslClientKeyArn { get; set; }

        [Input("sslClientKeyPassword")]
        private Input<string>? _sslClientKeyPassword;

        /// <summary>
        /// Password for the client private key used to securely connect to a Kafka target endpoint.
        /// </summary>
        public Input<string>? SslClientKeyPassword
        {
            get => _sslClientKeyPassword;
            set
            {
                var emptySecret = Output.CreateSecret(0);
                _sslClientKeyPassword = Output.Tuple<Input<string>?, int>(value, emptySecret).Apply(t => t.Item1);
            }
        }

        /// <summary>
        /// Kafka topic for migration. Default is `kafka-default-topic`.
        /// </summary>
        [Input("topic")]
        public Input<string>? Topic { get; set; }

        public EndpointKafkaSettingsArgs()
        {
        }
        public static new EndpointKafkaSettingsArgs Empty => new EndpointKafkaSettingsArgs();
    }
}
